{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /home/z/miniconda3/envs/mmxai/lib/python3.12/site-packages (2.3.0)\n",
      "Requirement already satisfied: torchvision in /home/z/miniconda3/envs/mmxai/lib/python3.12/site-packages (0.18.0)\n",
      "Collecting timm\n",
      "  Downloading timm-1.0.7-py3-none-any.whl.metadata (47 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.5/47.5 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: lime in /home/z/miniconda3/envs/mmxai/lib/python3.12/site-packages (0.2.0.1)\n",
      "Requirement already satisfied: shap in /home/z/miniconda3/envs/mmxai/lib/python3.12/site-packages (0.46.0)\n",
      "Requirement already satisfied: matplotlib in /home/z/miniconda3/envs/mmxai/lib/python3.12/site-packages (3.8.4)\n",
      "Requirement already satisfied: filelock in /home/z/miniconda3/envs/mmxai/lib/python3.12/site-packages (from torch) (3.14.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /home/z/miniconda3/envs/mmxai/lib/python3.12/site-packages (from torch) (4.9.0)\n",
      "Requirement already satisfied: sympy in /home/z/miniconda3/envs/mmxai/lib/python3.12/site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in /home/z/miniconda3/envs/mmxai/lib/python3.12/site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in /home/z/miniconda3/envs/mmxai/lib/python3.12/site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /home/z/miniconda3/envs/mmxai/lib/python3.12/site-packages (from torch) (2024.3.1)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /home/z/miniconda3/envs/mmxai/lib/python3.12/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /home/z/miniconda3/envs/mmxai/lib/python3.12/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /home/z/miniconda3/envs/mmxai/lib/python3.12/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /home/z/miniconda3/envs/mmxai/lib/python3.12/site-packages (from torch) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /home/z/miniconda3/envs/mmxai/lib/python3.12/site-packages (from torch) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /home/z/miniconda3/envs/mmxai/lib/python3.12/site-packages (from torch) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /home/z/miniconda3/envs/mmxai/lib/python3.12/site-packages (from torch) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /home/z/miniconda3/envs/mmxai/lib/python3.12/site-packages (from torch) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /home/z/miniconda3/envs/mmxai/lib/python3.12/site-packages (from torch) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /home/z/miniconda3/envs/mmxai/lib/python3.12/site-packages (from torch) (2.20.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /home/z/miniconda3/envs/mmxai/lib/python3.12/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /home/z/miniconda3/envs/mmxai/lib/python3.12/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.4.127)\n",
      "Requirement already satisfied: numpy in /home/z/miniconda3/envs/mmxai/lib/python3.12/site-packages (from torchvision) (1.26.4)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /home/z/miniconda3/envs/mmxai/lib/python3.12/site-packages (from torchvision) (10.2.0)\n",
      "Requirement already satisfied: pyyaml in /home/z/miniconda3/envs/mmxai/lib/python3.12/site-packages (from timm) (6.0.1)\n",
      "Requirement already satisfied: huggingface_hub in /home/z/miniconda3/envs/mmxai/lib/python3.12/site-packages (from timm) (0.23.0)\n",
      "Requirement already satisfied: safetensors in /home/z/miniconda3/envs/mmxai/lib/python3.12/site-packages (from timm) (0.4.3)\n",
      "Requirement already satisfied: scipy in /home/z/miniconda3/envs/mmxai/lib/python3.12/site-packages (from lime) (1.13.0)\n",
      "Requirement already satisfied: tqdm in /home/z/miniconda3/envs/mmxai/lib/python3.12/site-packages (from lime) (4.66.4)\n",
      "Requirement already satisfied: scikit-learn>=0.18 in /home/z/miniconda3/envs/mmxai/lib/python3.12/site-packages (from lime) (1.4.2)\n",
      "Requirement already satisfied: scikit-image>=0.12 in /home/z/.local/lib/python3.12/site-packages (from lime) (0.23.2)\n",
      "Requirement already satisfied: pandas in /home/z/miniconda3/envs/mmxai/lib/python3.12/site-packages (from shap) (2.2.2)\n",
      "Requirement already satisfied: packaging>20.9 in /home/z/miniconda3/envs/mmxai/lib/python3.12/site-packages (from shap) (23.2)\n",
      "Requirement already satisfied: slicer==0.0.8 in /home/z/miniconda3/envs/mmxai/lib/python3.12/site-packages (from shap) (0.0.8)\n",
      "Requirement already satisfied: numba in /home/z/miniconda3/envs/mmxai/lib/python3.12/site-packages (from shap) (0.60.0)\n",
      "Requirement already satisfied: cloudpickle in /home/z/miniconda3/envs/mmxai/lib/python3.12/site-packages (from shap) (3.0.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/z/miniconda3/envs/mmxai/lib/python3.12/site-packages (from matplotlib) (1.2.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/z/miniconda3/envs/mmxai/lib/python3.12/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/z/miniconda3/envs/mmxai/lib/python3.12/site-packages (from matplotlib) (4.51.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /home/z/miniconda3/envs/mmxai/lib/python3.12/site-packages (from matplotlib) (1.4.5)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /home/z/miniconda3/envs/mmxai/lib/python3.12/site-packages (from matplotlib) (3.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /home/z/miniconda3/envs/mmxai/lib/python3.12/site-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in /home/z/miniconda3/envs/mmxai/lib/python3.12/site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Requirement already satisfied: imageio>=2.33 in /home/z/.local/lib/python3.12/site-packages (from scikit-image>=0.12->lime) (2.34.1)\n",
      "Requirement already satisfied: tifffile>=2022.8.12 in /home/z/.local/lib/python3.12/site-packages (from scikit-image>=0.12->lime) (2024.5.22)\n",
      "Requirement already satisfied: lazy-loader>=0.4 in /home/z/.local/lib/python3.12/site-packages (from scikit-image>=0.12->lime) (0.4)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /home/z/miniconda3/envs/mmxai/lib/python3.12/site-packages (from scikit-learn>=0.18->lime) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/z/miniconda3/envs/mmxai/lib/python3.12/site-packages (from scikit-learn>=0.18->lime) (3.5.0)\n",
      "Requirement already satisfied: requests in /home/z/miniconda3/envs/mmxai/lib/python3.12/site-packages (from huggingface_hub->timm) (2.31.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/z/miniconda3/envs/mmxai/lib/python3.12/site-packages (from jinja2->torch) (2.1.5)\n",
      "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /home/z/miniconda3/envs/mmxai/lib/python3.12/site-packages (from numba->shap) (0.43.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/z/miniconda3/envs/mmxai/lib/python3.12/site-packages (from pandas->shap) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/z/miniconda3/envs/mmxai/lib/python3.12/site-packages (from pandas->shap) (2024.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in /home/z/miniconda3/envs/mmxai/lib/python3.12/site-packages (from sympy->torch) (1.3.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/z/miniconda3/envs/mmxai/lib/python3.12/site-packages (from requests->huggingface_hub->timm) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/z/miniconda3/envs/mmxai/lib/python3.12/site-packages (from requests->huggingface_hub->timm) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/z/miniconda3/envs/mmxai/lib/python3.12/site-packages (from requests->huggingface_hub->timm) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/z/miniconda3/envs/mmxai/lib/python3.12/site-packages (from requests->huggingface_hub->timm) (2024.2.2)\n",
      "Downloading timm-1.0.7-py3-none-any.whl (2.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m27.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: timm\n",
      "Successfully installed timm-1.0.7\n"
     ]
    }
   ],
   "source": [
    "!pip install torch torchvision timm lime shap matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grey whale 0.873874306678772\n",
      "tiger shark 0.007908593863248825\n",
      "leatherback turtle 0.007705869618803263\n",
      "great white shark 0.006708950735628605\n",
      "dugong 0.002876007230952382\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a277b5f951641c39f4fa35027e8a33a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/z/miniconda3/envs/mmxai/lib/python3.12/site-packages/shap/explainers/_deep/deep_pytorch.py:243: UserWarning: unrecognized nn.Module: Identity\n",
      "  warnings.warn(f'unrecognized nn.Module: {module_type}')\n",
      "/home/z/miniconda3/envs/mmxai/lib/python3.12/site-packages/shap/explainers/_deep/deep_pytorch.py:243: UserWarning: unrecognized nn.Module: LayerNorm\n",
      "  warnings.warn(f'unrecognized nn.Module: {module_type}')\n",
      "/home/z/miniconda3/envs/mmxai/lib/python3.12/site-packages/shap/explainers/_deep/deep_pytorch.py:243: UserWarning: unrecognized nn.Module: GELU\n",
      "  warnings.warn(f'unrecognized nn.Module: {module_type}')\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "The SHAP explanations do not sum up to the model's output! This is either because of a rounding error or because an operator in your computation graph was not fully supported. If the sum difference of %f is significant compared to the scale of your model outputs, please post as a github issue, with a reproducible example so we can debug it. Used framework: pytorch - Max. diff: 4.293602752022863 - Tolerance: 0.01",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 68\u001b[0m\n\u001b[1;32m     66\u001b[0m background \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros((\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m224\u001b[39m, \u001b[38;5;241m224\u001b[39m))\n\u001b[1;32m     67\u001b[0m e \u001b[38;5;241m=\u001b[39m shap\u001b[38;5;241m.\u001b[39mDeepExplainer(model, background)\n\u001b[0;32m---> 68\u001b[0m shap_values \u001b[38;5;241m=\u001b[39m \u001b[43me\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshap_values\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_tensor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;66;03m# 显示SHAP结果\u001b[39;00m\n\u001b[1;32m     71\u001b[0m shap\u001b[38;5;241m.\u001b[39mimage_plot(shap_values, \u001b[38;5;241m-\u001b[39minput_tensor\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy())\n",
      "File \u001b[0;32m~/miniconda3/envs/mmxai/lib/python3.12/site-packages/shap/explainers/_deep/__init__.py:159\u001b[0m, in \u001b[0;36mDeepExplainer.shap_values\u001b[0;34m(self, X, ranked_outputs, output_rank_order, check_additivity)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mshap_values\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, ranked_outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, output_rank_order\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmax\u001b[39m\u001b[38;5;124m'\u001b[39m, check_additivity\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m    116\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return approximate SHAP values for the model applied to the data given by X.\u001b[39;00m\n\u001b[1;32m    117\u001b[0m \n\u001b[1;32m    118\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    157\u001b[0m \n\u001b[1;32m    158\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 159\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexplainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshap_values\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mranked_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_rank_order\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheck_additivity\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcheck_additivity\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/mmxai/lib/python3.12/site-packages/shap/explainers/_deep/deep_pytorch.py:214\u001b[0m, in \u001b[0;36mPyTorchDeep.shap_values\u001b[0;34m(self, X, ranked_outputs, output_rank_order, check_additivity)\u001b[0m\n\u001b[1;32m    211\u001b[0m         \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m    212\u001b[0m             model_output_values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(\u001b[38;5;241m*\u001b[39mX)\n\u001b[0;32m--> 214\u001b[0m     \u001b[43m_check_additivity\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_output_values\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_phis\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    216\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(output_phis, \u001b[38;5;28mlist\u001b[39m):\n\u001b[1;32m    217\u001b[0m     \u001b[38;5;66;03m# in this case we have multiple inputs and potentially multiple outputs\u001b[39;00m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(output_phis[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;28mlist\u001b[39m):\n",
      "File \u001b[0;32m~/miniconda3/envs/mmxai/lib/python3.12/site-packages/shap/explainers/_deep/deep_utils.py:20\u001b[0m, in \u001b[0;36m_check_additivity\u001b[0;34m(explainer, model_output_values, output_phis)\u001b[0m\n\u001b[1;32m     16\u001b[0m         diffs \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m output_phis[t][i]\u001b[38;5;241m.\u001b[39msum(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mtuple\u001b[39m(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, output_phis[t][i]\u001b[38;5;241m.\u001b[39mndim)))\n\u001b[1;32m     18\u001b[0m maxdiff \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mabs(diffs)\u001b[38;5;241m.\u001b[39mmax()\n\u001b[0;32m---> 20\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m maxdiff \u001b[38;5;241m<\u001b[39m TOLERANCE, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe SHAP explanations do not sum up to the model\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms output! This is either because of a \u001b[39m\u001b[38;5;124m\"\u001b[39m \\\n\u001b[1;32m     21\u001b[0m                             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrounding error or because an operator in your computation graph was not fully supported. If \u001b[39m\u001b[38;5;124m\"\u001b[39m \\\n\u001b[1;32m     22\u001b[0m                             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthe sum difference of \u001b[39m\u001b[38;5;132;01m%f\u001b[39;00m\u001b[38;5;124m is significant compared to the scale of your model outputs, please post \u001b[39m\u001b[38;5;124m\"\u001b[39m \\\n\u001b[1;32m     23\u001b[0m                             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas a github issue, with a reproducible example so we can debug it. Used framework: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexplainer\u001b[38;5;241m.\u001b[39mframework\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m - Max. diff: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmaxdiff\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m - Tolerance: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mTOLERANCE\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;31mAssertionError\u001b[0m: The SHAP explanations do not sum up to the model's output! This is either because of a rounding error or because an operator in your computation graph was not fully supported. If the sum difference of %f is significant compared to the scale of your model outputs, please post as a github issue, with a reproducible example so we can debug it. Used framework: pytorch - Max. diff: 4.293602752022863 - Tolerance: 0.01"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from lime import lime_image\n",
    "from lime.wrappers.scikit_image import SegmentationAlgorithm\n",
    "import shap\n",
    "import timm\n",
    "from torchvision.models import ViT_B_16_Weights\n",
    "\n",
    "# 加载预训练的ViT模型\n",
    "model = timm.create_model('vit_base_patch16_224', pretrained=True)\n",
    "model.eval()\n",
    "\n",
    "# 图像预处理\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "# 加载图像类别\n",
    "categories = ViT_B_16_Weights.IMAGENET1K_V1.meta[\"categories\"]\n",
    "\n",
    "# 加载本地图像\n",
    "image_path = 'n01491361/ILSVRC2012_val_00028564_n01491361.JPEG'\n",
    "image = Image.open(image_path).convert('RGB')\n",
    "input_tensor = preprocess(image).unsqueeze(0)\n",
    "\n",
    "# 模型预测\n",
    "with torch.no_grad():\n",
    "    output = model(input_tensor)\n",
    "    \n",
    "probabilities = torch.nn.functional.softmax(output[0], dim=0)\n",
    "top5_prob, top5_catid = torch.topk(probabilities, 5)\n",
    "\n",
    "for i in range(top5_prob.size(0)):\n",
    "    print(categories[top5_catid[i]], top5_prob[i].item())\n",
    "\n",
    "# 定义预测函数\n",
    "def predict(images):\n",
    "    if isinstance(images, np.ndarray):\n",
    "        images = [Image.fromarray((img * 255).astype(np.uint8)) for img in images]\n",
    "    batch = torch.stack(tuple(preprocess(i) for i in images), dim=0)\n",
    "    predictions = model(batch)\n",
    "    return predictions.detach().numpy()\n",
    "\n",
    "# LIME解释\n",
    "explainer = lime_image.LimeImageExplainer()\n",
    "explanation = explainer.explain_instance(np.array(image), \n",
    "                                         predict, \n",
    "                                         top_labels=5, \n",
    "                                         hide_color=0, \n",
    "                                         num_samples=1000)\n",
    "\n",
    "# 显示LIME结果\n",
    "from skimage.segmentation import mark_boundaries\n",
    "\n",
    "temp, mask = explanation.get_image_and_mask(explanation.top_labels[0], positive_only=True, num_features=5, hide_rest=True)\n",
    "plt.imshow(mark_boundaries(temp / 2 + 0.5, mask))\n",
    "plt.title('LIME Explanation')\n",
    "plt.show()\n",
    "\n",
    "# SHAP解释\n",
    "background = torch.zeros((1, 3, 224, 224))\n",
    "e = shap.DeepExplainer(model, background)\n",
    "shap_values = e.shap_values(input_tensor)\n",
    "\n",
    "# 显示SHAP结果\n",
    "shap.image_plot(shap_values, -input_tensor.cpu().numpy())\n",
    "plt.title('SHAP Explanation')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224-in21k and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted label: 1, Probability: 0.5460480451583862\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd11b9f613d045ee95c92d2b9c8b10fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ValueError",
     "evalue": "Make sure that the channel dimension of the pixel values match with the one set in the configuration. Expected 3 but got 550.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 45\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;66;03m# LIME explanation\u001b[39;00m\n\u001b[1;32m     44\u001b[0m explainer \u001b[38;5;241m=\u001b[39m lime_image\u001b[38;5;241m.\u001b[39mLimeImageExplainer()\n\u001b[0;32m---> 45\u001b[0m explanation \u001b[38;5;241m=\u001b[39m \u001b[43mexplainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexplain_instance\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpredict_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_labels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhide_color\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;66;03m# Plot LIME explanation\u001b[39;00m\n\u001b[1;32m     48\u001b[0m temp, mask \u001b[38;5;241m=\u001b[39m explanation\u001b[38;5;241m.\u001b[39mget_image_and_mask(top_labels[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mitem(), positive_only\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, num_features\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, hide_rest\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m~/miniconda3/envs/mmxai/lib/python3.12/site-packages/lime/lime_image.py:198\u001b[0m, in \u001b[0;36mLimeImageExplainer.explain_instance\u001b[0;34m(self, image, classifier_fn, labels, hide_color, top_labels, num_features, num_samples, batch_size, segmentation_fn, distance_metric, model_regressor, random_seed)\u001b[0m\n\u001b[1;32m    194\u001b[0m     fudged_image[:] \u001b[38;5;241m=\u001b[39m hide_color\n\u001b[1;32m    196\u001b[0m top \u001b[38;5;241m=\u001b[39m labels\n\u001b[0;32m--> 198\u001b[0m data, labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata_labels\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfudged_image\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msegments\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[43m                                \u001b[49m\u001b[43mclassifier_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_samples\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    200\u001b[0m \u001b[43m                                \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    202\u001b[0m distances \u001b[38;5;241m=\u001b[39m sklearn\u001b[38;5;241m.\u001b[39mmetrics\u001b[38;5;241m.\u001b[39mpairwise_distances(\n\u001b[1;32m    203\u001b[0m     data,\n\u001b[1;32m    204\u001b[0m     data[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m),\n\u001b[1;32m    205\u001b[0m     metric\u001b[38;5;241m=\u001b[39mdistance_metric\n\u001b[1;32m    206\u001b[0m )\u001b[38;5;241m.\u001b[39mravel()\n\u001b[1;32m    208\u001b[0m ret_exp \u001b[38;5;241m=\u001b[39m ImageExplanation(image, segments)\n",
      "File \u001b[0;32m~/miniconda3/envs/mmxai/lib/python3.12/site-packages/lime/lime_image.py:261\u001b[0m, in \u001b[0;36mLimeImageExplainer.data_labels\u001b[0;34m(self, image, fudged_image, segments, classifier_fn, num_samples, batch_size)\u001b[0m\n\u001b[1;32m    259\u001b[0m imgs\u001b[38;5;241m.\u001b[39mappend(temp)\n\u001b[1;32m    260\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(imgs) \u001b[38;5;241m==\u001b[39m batch_size:\n\u001b[0;32m--> 261\u001b[0m     preds \u001b[38;5;241m=\u001b[39m \u001b[43mclassifier_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimgs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    262\u001b[0m     labels\u001b[38;5;241m.\u001b[39mextend(preds)\n\u001b[1;32m    263\u001b[0m     imgs \u001b[38;5;241m=\u001b[39m []\n",
      "Cell \u001b[0;32mIn[11], line 39\u001b[0m, in \u001b[0;36mpredict_fn\u001b[0;34m(images)\u001b[0m\n\u001b[1;32m     37\u001b[0m inputs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(images)\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 39\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpixel_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     40\u001b[0m probs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39msoftmax(outputs\u001b[38;5;241m.\u001b[39mlogits, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m probs\n",
      "File \u001b[0;32m~/miniconda3/envs/mmxai/lib/python3.12/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/mmxai/lib/python3.12/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/mmxai/lib/python3.12/site-packages/transformers/models/vit/modeling_vit.py:791\u001b[0m, in \u001b[0;36mViTForImageClassification.forward\u001b[0;34m(self, pixel_values, head_mask, labels, output_attentions, output_hidden_states, interpolate_pos_encoding, return_dict)\u001b[0m\n\u001b[1;32m    783\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    784\u001b[0m \u001b[38;5;124;03mlabels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\u001b[39;00m\n\u001b[1;32m    785\u001b[0m \u001b[38;5;124;03m    Labels for computing the image classification/regression loss. Indices should be in `[0, ...,\u001b[39;00m\n\u001b[1;32m    786\u001b[0m \u001b[38;5;124;03m    config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\u001b[39;00m\n\u001b[1;32m    787\u001b[0m \u001b[38;5;124;03m    `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\u001b[39;00m\n\u001b[1;32m    788\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    789\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[0;32m--> 791\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    792\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpixel_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    793\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    794\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    795\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    796\u001b[0m \u001b[43m    \u001b[49m\u001b[43minterpolate_pos_encoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minterpolate_pos_encoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    797\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    798\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    800\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    802\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclassifier(sequence_output[:, \u001b[38;5;241m0\u001b[39m, :])\n",
      "File \u001b[0;32m~/miniconda3/envs/mmxai/lib/python3.12/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/mmxai/lib/python3.12/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/mmxai/lib/python3.12/site-packages/transformers/models/vit/modeling_vit.py:570\u001b[0m, in \u001b[0;36mViTModel.forward\u001b[0;34m(self, pixel_values, bool_masked_pos, head_mask, output_attentions, output_hidden_states, interpolate_pos_encoding, return_dict)\u001b[0m\n\u001b[1;32m    567\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pixel_values\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m!=\u001b[39m expected_dtype:\n\u001b[1;32m    568\u001b[0m     pixel_values \u001b[38;5;241m=\u001b[39m pixel_values\u001b[38;5;241m.\u001b[39mto(expected_dtype)\n\u001b[0;32m--> 570\u001b[0m embedding_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membeddings\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    571\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpixel_values\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbool_masked_pos\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbool_masked_pos\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minterpolate_pos_encoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minterpolate_pos_encoding\u001b[49m\n\u001b[1;32m    572\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    574\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder(\n\u001b[1;32m    575\u001b[0m     embedding_output,\n\u001b[1;32m    576\u001b[0m     head_mask\u001b[38;5;241m=\u001b[39mhead_mask,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    579\u001b[0m     return_dict\u001b[38;5;241m=\u001b[39mreturn_dict,\n\u001b[1;32m    580\u001b[0m )\n\u001b[1;32m    581\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/miniconda3/envs/mmxai/lib/python3.12/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/mmxai/lib/python3.12/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/mmxai/lib/python3.12/site-packages/transformers/models/vit/modeling_vit.py:119\u001b[0m, in \u001b[0;36mViTEmbeddings.forward\u001b[0;34m(self, pixel_values, bool_masked_pos, interpolate_pos_encoding)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    113\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    114\u001b[0m     pixel_values: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[1;32m    115\u001b[0m     bool_masked_pos: Optional[torch\u001b[38;5;241m.\u001b[39mBoolTensor] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    116\u001b[0m     interpolate_pos_encoding: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    117\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[1;32m    118\u001b[0m     batch_size, num_channels, height, width \u001b[38;5;241m=\u001b[39m pixel_values\u001b[38;5;241m.\u001b[39mshape\n\u001b[0;32m--> 119\u001b[0m     embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpatch_embeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpixel_values\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minterpolate_pos_encoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minterpolate_pos_encoding\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m bool_masked_pos \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    122\u001b[0m         seq_length \u001b[38;5;241m=\u001b[39m embeddings\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\n",
      "File \u001b[0;32m~/miniconda3/envs/mmxai/lib/python3.12/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/mmxai/lib/python3.12/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/mmxai/lib/python3.12/site-packages/transformers/models/vit/modeling_vit.py:168\u001b[0m, in \u001b[0;36mViTPatchEmbeddings.forward\u001b[0;34m(self, pixel_values, interpolate_pos_encoding)\u001b[0m\n\u001b[1;32m    166\u001b[0m batch_size, num_channels, height, width \u001b[38;5;241m=\u001b[39m pixel_values\u001b[38;5;241m.\u001b[39mshape\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m num_channels \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_channels:\n\u001b[0;32m--> 168\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    169\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMake sure that the channel dimension of the pixel values match with the one set in the configuration.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    170\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m Expected \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_channels\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_channels\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    171\u001b[0m     )\n\u001b[1;32m    172\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m interpolate_pos_encoding:\n\u001b[1;32m    173\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m height \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimage_size[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;129;01mor\u001b[39;00m width \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimage_size[\u001b[38;5;241m1\u001b[39m]:\n",
      "\u001b[0;31mValueError\u001b[0m: Make sure that the channel dimension of the pixel values match with the one set in the configuration. Expected 3 but got 550."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import ViTForImageClassification, ViTFeatureExtractor\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from lime import lime_image\n",
    "import shap\n",
    "\n",
    "# Load the pre-trained ViT model and feature extractor\n",
    "model_name = 'google/vit-base-patch16-224-in21k'\n",
    "model = ViTForImageClassification.from_pretrained(model_name)\n",
    "feature_extractor = ViTFeatureExtractor.from_pretrained(model_name)\n",
    "\n",
    "# Load your local image\n",
    "img_path = 'n01491361/ILSVRC2012_val_00028564_n01491361.JPEG'  # Replace with your image path\n",
    "image = Image.open(img_path).convert('RGB')\n",
    "\n",
    "# Preprocess the image\n",
    "inputs = feature_extractor(images=image, return_tensors=\"pt\")\n",
    "\n",
    "# Predict\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "    probs = torch.nn.functional.softmax(outputs.logits, dim=1)\n",
    "    top_probs, top_labels = torch.topk(probs, 1)\n",
    "\n",
    "# Print the top predicted label and probability\n",
    "print(f\"Predicted label: {top_labels[0].item()}, Probability: {top_probs[0].item()}\")\n",
    "\n",
    "# Function to preprocess images for LIME\n",
    "def preprocess_fn(images):\n",
    "    return feature_extractor(images=[Image.fromarray(image).convert('RGB') for image in images], return_tensors=\"pt\").pixel_values.numpy()\n",
    "\n",
    "# Function to predict using the model for LIME\n",
    "def predict_fn(images):\n",
    "    inputs = torch.tensor(images)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(pixel_values=inputs)\n",
    "    probs = torch.nn.functional.softmax(outputs.logits, dim=1).numpy()\n",
    "    return probs\n",
    "\n",
    "# LIME explanation\n",
    "explainer = lime_image.LimeImageExplainer()\n",
    "explanation = explainer.explain_instance(np.array(image), predict_fn, top_labels=5, hide_color=0, num_samples=1000)\n",
    "\n",
    "# Plot LIME explanation\n",
    "temp, mask = explanation.get_image_and_mask(top_labels[0].item(), positive_only=True, num_features=10, hide_rest=False)\n",
    "plt.imshow(temp)\n",
    "plt.title('LIME Explanation')\n",
    "plt.show()\n",
    "\n",
    "# SHAP explanation\n",
    "def shap_predict(images):\n",
    "    inputs = torch.tensor(images).to(model.device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(pixel_values=inputs)\n",
    "    return outputs.logits\n",
    "\n",
    "background = torch.cat([feature_extractor(images=image, return_tensors=\"pt\").pixel_values for _ in range(50)], dim=0).numpy()\n",
    "e = shap.DeepExplainer(shap_predict, background)\n",
    "shap_values = e.shap_values(feature_extractor(images=image, return_tensors=\"pt\").pixel_values.numpy())\n",
    "\n",
    "# Plot SHAP explanation\n",
    "shap.image_plot(shap_values, np.array([image]))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mmxai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
