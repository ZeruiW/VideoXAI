{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train ViT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/z/miniconda3/envs/mmxai/lib/python3.12/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224-in21k and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 5.392894744873047\n",
      "Epoch 2, Loss: 4.374405860900879\n",
      "Epoch 3, Loss: 2.4989871978759766\n",
      "Epoch 4, Loss: 2.265422821044922\n",
      "Epoch 5, Loss: 0.9371886253356934\n",
      "Accuracy: 0.8917\n",
      "Precision: 0.9100\n",
      "Recall: 0.8917\n",
      "F1 Score: 0.8870\n"
     ]
    }
   ],
   "source": [
    "# import os\n",
    "# import av\n",
    "# import torch\n",
    "# import numpy as np\n",
    "# import random\n",
    "# import json\n",
    "# import time\n",
    "# import subprocess\n",
    "# import psutil\n",
    "# from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "# from transformers import ViTForImageClassification, ViTImageProcessor\n",
    "# from collections import defaultdict\n",
    "# from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "# # 定义配置\n",
    "# config = {\n",
    "#     \"model_name\": \"google/vit-base-patch16-224-in21k\",\n",
    "#     \"image_processor_name\": \"google/vit-base-patch16-224-in21k\",\n",
    "#     \"num_samples\": 10000,\n",
    "#     \"num_classes\": 400,  # For flexible dataset input\n",
    "#     \"num_samples_per_class\": 25,  # For flexible dataset input\n",
    "#     \"video_list_path\": \"archive/kinetics400_val_list_videos.txt\",\n",
    "#     \"video_directory\": \"archive/zoom_blur\",\n",
    "#     \"use_exact\": True\n",
    "# }\n",
    "\n",
    "# # 加载预训练的ViT模型和处理器\n",
    "# model = ViTForImageClassification.from_pretrained(config[\"model_name\"], num_labels=400)\n",
    "# image_processor = ViTImageProcessor.from_pretrained(config[\"image_processor_name\"])\n",
    "\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# model.to(device)\n",
    "\n",
    "# # 读取视频文件和标签\n",
    "# video_labels = defaultdict(list)\n",
    "# with open(config[\"video_list_path\"], \"r\") as f:\n",
    "#     for line in f:\n",
    "#         name, label = line.strip().split()\n",
    "#         video_labels[int(label)].append(name)\n",
    "\n",
    "# # 准备样本\n",
    "# sampled_files = []\n",
    "# true_labels = []\n",
    "# selected_classes = random.sample(list(video_labels.keys()), config[\"num_classes\"])\n",
    "# for cls in selected_classes:\n",
    "#     sampled_files.extend(random.sample(video_labels[cls], config[\"num_samples_per_class\"]))\n",
    "#     true_labels.extend([cls] * config[\"num_samples_per_class\"])\n",
    "\n",
    "# # 自定义数据集类\n",
    "# class Kinetics400Dataset(Dataset):\n",
    "#     def __init__(self, video_files, labels, image_processor):\n",
    "#         self.video_files = video_files\n",
    "#         self.labels = labels\n",
    "#         self.image_processor = image_processor\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.video_files)\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         video_file = self.video_files[idx]\n",
    "#         label = self.labels[idx]\n",
    "#         container = av.open(os.path.join(config[\"video_directory\"], video_file))\n",
    "#         frame = container.decode(video=0).__next__().to_image()  # 获取第一帧\n",
    "#         inputs = self.image_processor(images=frame, return_tensors=\"pt\")\n",
    "#         inputs = {k: v.squeeze(0) for k, v in inputs.items()}\n",
    "#         return inputs['pixel_values'], torch.tensor(label)\n",
    "\n",
    "# # 准备数据集和数据加载器\n",
    "# dataset = Kinetics400Dataset(sampled_files, true_labels, image_processor)\n",
    "# dataloader = DataLoader(dataset, batch_size=8, shuffle=True)\n",
    "\n",
    "# # 微调模型\n",
    "# optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)\n",
    "# criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# model.train()\n",
    "# for epoch in range(5):  # 你可以根据需要调整epoch数量\n",
    "#     for batch in dataloader:\n",
    "#         images, labels = batch\n",
    "#         images = images.to(device)\n",
    "#         labels = labels.to(device)\n",
    "\n",
    "#         optimizer.zero_grad()\n",
    "#         outputs = model(images).logits\n",
    "#         loss = criterion(outputs, labels)\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "\n",
    "#     print(f\"Epoch {epoch + 1}, Loss: {loss.item()}\")\n",
    "\n",
    "# # 预测函数\n",
    "# model.eval()\n",
    "# def predict_video(video_file):\n",
    "#     container = av.open(os.path.join(config[\"video_directory\"], video_file))\n",
    "#     frame = container.decode(video=0).__next__().to_image()  # 获取第一帧\n",
    "#     inputs = image_processor(images=frame, return_tensors=\"pt\")\n",
    "#     inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "#     with torch.no_grad():\n",
    "#         outputs = model(**inputs).logits\n",
    "#         predicted_label = torch.argmax(outputs, dim=-1).item()\n",
    "#     return predicted_label\n",
    "\n",
    "# # 对样本进行预测并评估\n",
    "# predictions = [predict_video(video_file) for video_file in sampled_files]\n",
    "\n",
    "# accuracy = accuracy_score(true_labels, predictions)\n",
    "# precision = precision_score(true_labels, predictions, average='weighted')\n",
    "# recall = recall_score(true_labels, predictions, average='weighted')\n",
    "# f1 = f1_score(true_labels, predictions, average='weighted')\n",
    "\n",
    "# print(f\"Accuracy: {accuracy:.4f}\")\n",
    "# print(f\"Precision: {precision:.4f}\")\n",
    "# print(f\"Recall: {recall:.4f}\")\n",
    "# print(f\"F1 Score: {f1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train ViT model and save the finetune results ONLY work on the first frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224-in21k and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 74\u001b[0m\n\u001b[1;32m     72\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m5\u001b[39m):  \u001b[38;5;66;03m# 你可以根据需要调整epoch数量\u001b[39;00m\n\u001b[0;32m---> 74\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m     75\u001b[0m \u001b[43m        \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\n\u001b[1;32m     76\u001b[0m \u001b[43m        \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mimages\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/mmxai/lib/python3.12/site-packages/torch/utils/data/dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/miniconda3/envs/mmxai/lib/python3.12/site-packages/torch/utils/data/dataloader.py:675\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    674\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 675\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    676\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    677\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/miniconda3/envs/mmxai/lib/python3.12/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[0;32mIn[18], line 59\u001b[0m, in \u001b[0;36mKinetics400Dataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     57\u001b[0m label \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabels[idx]\n\u001b[1;32m     58\u001b[0m container \u001b[38;5;241m=\u001b[39m av\u001b[38;5;241m.\u001b[39mopen(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvideo_directory\u001b[39m\u001b[38;5;124m\"\u001b[39m], video_file))\n\u001b[0;32m---> 59\u001b[0m frame \u001b[38;5;241m=\u001b[39m \u001b[43mcontainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvideo\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__next__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto_image()  \u001b[38;5;66;03m# 获取第一帧\u001b[39;00m\n\u001b[1;32m     60\u001b[0m inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimage_processor(images\u001b[38;5;241m=\u001b[39mframe, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     61\u001b[0m inputs \u001b[38;5;241m=\u001b[39m {k: v\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m0\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m inputs\u001b[38;5;241m.\u001b[39mitems()}\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import av\n",
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from transformers import ViTForImageClassification, ViTImageProcessor\n",
    "from collections import defaultdict\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "# 定义配置\n",
    "config = {\n",
    "    \"model_name\": \"google/vit-base-patch16-224-in21k\",\n",
    "    \"image_processor_name\": \"google/vit-base-patch16-224-in21k\",\n",
    "    \"num_samples\": 10000,\n",
    "    \"num_classes\": 400,  # For flexible dataset input\n",
    "    \"num_samples_per_class\": 25,  # For flexible dataset input\n",
    "    \"video_list_path\": \"archive/kinetics400_val_list_videos.txt\",\n",
    "    \"video_directory\": \"archive/videos_val\",\n",
    "    \"use_exact\": True\n",
    "}\n",
    "\n",
    "# 加载预训练的ViT模型和处理器\n",
    "model = ViTForImageClassification.from_pretrained(config[\"model_name\"], num_labels=400)\n",
    "image_processor = ViTImageProcessor.from_pretrained(config[\"image_processor_name\"])\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# 读取视频文件和标签\n",
    "video_labels = defaultdict(list)\n",
    "with open(config[\"video_list_path\"], \"r\") as f:\n",
    "    for line in f:\n",
    "        name, label = line.strip().split()\n",
    "        video_labels[int(label)].append(name)\n",
    "\n",
    "# 准备样本\n",
    "sampled_files = []\n",
    "true_labels = []\n",
    "selected_classes = random.sample(list(video_labels.keys()), config[\"num_classes\"])\n",
    "for cls in selected_classes:\n",
    "    sampled_files.extend(random.sample(video_labels[cls], config[\"num_samples_per_class\"]))\n",
    "    true_labels.extend([cls] * config[\"num_samples_per_class\"])\n",
    "\n",
    "# 自定义数据集类\n",
    "class Kinetics400Dataset(Dataset):\n",
    "    def __init__(self, video_files, labels, image_processor):\n",
    "        self.video_files = video_files\n",
    "        self.labels = labels\n",
    "        self.image_processor = image_processor\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.video_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        video_file = self.video_files[idx]\n",
    "        label = self.labels[idx]\n",
    "        container = av.open(os.path.join(config[\"video_directory\"], video_file))\n",
    "        frame = container.decode(video=0).__next__().to_image()  # 获取第一帧\n",
    "        inputs = self.image_processor(images=frame, return_tensors=\"pt\")\n",
    "        inputs = {k: v.squeeze(0) for k, v in inputs.items()}\n",
    "        return inputs['pixel_values'], torch.tensor(label)\n",
    "\n",
    "# 准备数据集和数据加载器\n",
    "dataset = Kinetics400Dataset(sampled_files, true_labels, image_processor)\n",
    "dataloader = DataLoader(dataset, batch_size=8, shuffle=True)\n",
    "\n",
    "# 微调模型\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "model.train()\n",
    "for epoch in range(5):  # 你可以根据需要调整epoch数量\n",
    "    for batch in dataloader:\n",
    "        images, labels = batch\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images).logits\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}, Loss: {loss.item()}\")\n",
    "\n",
    "# 保存微调后的模型\n",
    "model_save_path = \"finetuned_vit_model5.pth\"\n",
    "torch.save(model.state_dict(), model_save_path)\n",
    "print(f\"Model saved to {model_save_path}\")\n",
    "\n",
    "# 定义推理函数\n",
    "def predict_video(video_file, model, image_processor):\n",
    "    container = av.open(video_file)\n",
    "    frame = container.decode(video=0).__next__().to_image()  # 获取第一帧\n",
    "    inputs = image_processor(images=frame, return_tensors=\"pt\")\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs).logits\n",
    "        predicted_label = torch.argmax(outputs, dim=-1).item()\n",
    "        probabilities = torch.nn.functional.softmax(outputs, dim=-1)\n",
    "        predicted_score = probabilities[0, predicted_label].item()\n",
    "\n",
    "    return predicted_label, predicted_score\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "work on 5 frame for each video, finetune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_files' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 22\u001b[0m\n\u001b[1;32m      4\u001b[0m config \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgoogle/vit-base-patch16-224-in21k\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimage_processor_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgoogle/vit-base-patch16-224-in21k\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpatience\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m5\u001b[39m  \u001b[38;5;66;03m# 用于早停\u001b[39;00m\n\u001b[1;32m     18\u001b[0m }\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# 数据分割\u001b[39;00m\n\u001b[1;32m     21\u001b[0m train_files, val_files, train_labels, val_labels \u001b[38;5;241m=\u001b[39m train_test_split(\n\u001b[0;32m---> 22\u001b[0m     \u001b[43mtrain_files\u001b[49m, train_labels, test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m, stratify\u001b[38;5;241m=\u001b[39mtrain_labels, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m\n\u001b[1;32m     23\u001b[0m )\n\u001b[1;32m     25\u001b[0m train_dataset \u001b[38;5;241m=\u001b[39m Kinetics400Dataset(train_files, train_labels, image_processor, config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mframes_per_video\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m     26\u001b[0m val_dataset \u001b[38;5;241m=\u001b[39m Kinetics400Dataset(val_files, val_labels, image_processor, config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mframes_per_video\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_files' is not defined"
     ]
    }
   ],
   "source": [
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "config = {\n",
    "    \"model_name\": \"google/vit-base-patch16-224-in21k\",\n",
    "    \"image_processor_name\": \"google/vit-base-patch16-224-in21k\",\n",
    "    \"num_samples\": 10000,\n",
    "    \"num_classes\": 400,\n",
    "    \"num_samples_per_class\": 25,\n",
    "    \"video_list_path\": \"archive/kinetics400_val_list_videos.txt\",\n",
    "    \"video_directory\": \"archive/videos_val\",\n",
    "    \"use_exact\": True,\n",
    "    \"frames_per_video\": 25,\n",
    "    \"batch_size\": 64,\n",
    "    \"num_epochs\": 100,\n",
    "    \"learning_rate\": 1e-4,\n",
    "    \"patience\": 5  # 用于早停\n",
    "}\n",
    "\n",
    "# 数据分割\n",
    "train_files, val_files, train_labels, val_labels = train_test_split(\n",
    "    train_files, train_labels, test_size=0.2, stratify=train_labels, random_state=42\n",
    ")\n",
    "\n",
    "train_dataset = Kinetics400Dataset(train_files, train_labels, image_processor, config[\"frames_per_video\"])\n",
    "val_dataset = Kinetics400Dataset(val_files, val_labels, image_processor, config[\"frames_per_video\"])\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=config[\"batch_size\"], shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=config[\"batch_size\"])\n",
    "\n",
    "def train_model(model, train_dataloader, val_dataloader, num_epochs, patience):\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=config[\"learning_rate\"])\n",
    "    scheduler = ReduceLROnPlateau(optimizer, mode='max', factor=0.1, patience=3)\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    best_val_accuracy = 0\n",
    "    epochs_no_improve = 0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for batch in tqdm(train_dataloader, desc=f\"Epoch {epoch+1}/{num_epochs}\"):\n",
    "            images, labels = batch\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images).logits\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        avg_loss = total_loss / len(train_dataloader)\n",
    "        print(f\"Epoch {epoch + 1}, Average Loss: {avg_loss:.4f}\")\n",
    "\n",
    "        # Validation\n",
    "        val_accuracy = evaluate_model(model, val_dataloader)\n",
    "        print(f\"Validation Accuracy: {val_accuracy:.4f}\")\n",
    "\n",
    "        scheduler.step(val_accuracy)\n",
    "\n",
    "        if val_accuracy > best_val_accuracy:\n",
    "            best_val_accuracy = val_accuracy\n",
    "            epochs_no_improve = 0\n",
    "            torch.save(model.state_dict(), \"finetuned_vit_model_20.pth\")\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "            if epochs_no_improve == patience:\n",
    "                print(\"Early stopping!\")\n",
    "                break\n",
    "\n",
    "    model.load_state_dict(torch.load(\"finetuned_vit_model_20.pth\"))\n",
    "    return model\n",
    "\n",
    "# 修改评估函数以支持数据加载器\n",
    "def evaluate_model(model, dataloader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in dataloader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images).logits\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    return correct / total\n",
    "\n",
    "# 训练模型\n",
    "model = train_model(model, train_dataloader, val_dataloader, config[\"num_epochs\"], config[\"patience\"])\n",
    "\n",
    "# 在测试集上评估\n",
    "test_dataset = Kinetics400Dataset(test_files, test_labels, image_processor, config[\"frames_per_video\"])\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=config[\"batch_size\"])\n",
    "test_accuracy = evaluate_model(model, test_dataloader)\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 重新加载模型进行推理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224-in21k and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "deprecated pixel format used, make sure you did set range correctly\n",
      "deprecated pixel format used, make sure you did set range correctly\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.4901\n",
      "Precision: 0.5238\n",
      "Recall: 0.4901\n",
      "F1 Score: 0.4946\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import av\n",
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from transformers import ViTForImageClassification, ViTImageProcessor\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "# Reuse the config from the training script\n",
    "config = {\n",
    "    \"model_name\": \"google/vit-base-patch16-224-in21k\",\n",
    "    \"image_processor_name\": \"google/vit-base-patch16-224-in21k\",\n",
    "    \"num_samples\": 2000,\n",
    "    \"num_classes\": 400,\n",
    "    \"num_samples_per_class\": 5,\n",
    "    \"video_list_path\": \"archive/kinetics400_val_list_videos.txt\",\n",
    "    \"video_directory\": \"archive/videos_val\",\n",
    "    \"use_exact\": True\n",
    "}\n",
    "\n",
    "# Load the fine-tuned model\n",
    "model = ViTForImageClassification.from_pretrained(config[\"model_name\"], num_labels=400)\n",
    "model.load_state_dict(torch.load(\"finetuned_vit_model_20.pth\"))\n",
    "image_processor = ViTImageProcessor.from_pretrained(config[\"image_processor_name\"])\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# Modified Kinetics400Dataset to extract middle frame\n",
    "class Kinetics400Dataset(Dataset):\n",
    "    def __init__(self, video_files, labels, image_processor):\n",
    "        self.video_files = video_files\n",
    "        self.labels = labels\n",
    "        self.image_processor = image_processor\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.video_files)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        video_file = self.video_files[idx]\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        container = av.open(os.path.join(config[\"video_directory\"], video_file))\n",
    "        video = container.streams.video[0]\n",
    "        \n",
    "        # Get the middle frame\n",
    "        middle_frame_index = video.frames // 2\n",
    "        for i, frame in enumerate(container.decode(video=0)):\n",
    "            if i == middle_frame_index:\n",
    "                middle_frame = frame.to_image()\n",
    "                break\n",
    "        \n",
    "        inputs = self.image_processor(images=middle_frame, return_tensors=\"pt\")\n",
    "        inputs = {k: v.squeeze(0) for k, v in inputs.items()}\n",
    "        return inputs['pixel_values'], torch.tensor(label)\n",
    "\n",
    "# Prepare the evaluation dataset\n",
    "video_labels = {}\n",
    "with open(config[\"video_list_path\"], \"r\") as f:\n",
    "    for line in f:\n",
    "        name, label = line.strip().split()\n",
    "        video_labels[name] = int(label)\n",
    "\n",
    "eval_files = list(video_labels.keys())\n",
    "eval_labels = [video_labels[file] for file in eval_files]\n",
    "\n",
    "eval_dataset = Kinetics400Dataset(eval_files, eval_labels, image_processor)\n",
    "eval_dataloader = DataLoader(eval_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Evaluation\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in eval_dataloader:\n",
    "        images, labels = batch\n",
    "        images = images.to(device)\n",
    "        \n",
    "        outputs = model(images).logits\n",
    "        preds = torch.argmax(outputs, dim=-1).cpu().numpy()\n",
    "        \n",
    "        all_preds.extend(preds)\n",
    "        all_labels.extend(labels.numpy())\n",
    "\n",
    "# Calculate metrics\n",
    "accuracy = accuracy_score(all_labels, all_preds)\n",
    "precision = precision_score(all_labels, all_preds, average='weighted')\n",
    "recall = recall_score(all_labels, all_preds, average='weighted')\n",
    "f1 = f1_score(all_labels, all_preds, average='weighted')\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mmxai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
