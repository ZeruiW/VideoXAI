{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "STAA "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/z/miniconda3/envs/mmxai/lib/python3.12/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Processing videos:   0%|          | 0/1200 [00:00<?, ?it/s]/home/z/miniconda3/envs/mmxai/lib/python3.12/site-packages/transformers/feature_extraction_utils.py:141: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:274.)\n",
      "  return torch.tensor(value)\n",
      "Processing videos:   3%|▎         | 37/1200 [01:25<44:47,  2.31s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 152\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    145\u001b[0m     config \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    146\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel_name\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfacebook/timesformer-base-finetuned-k400\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m    147\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvideo_directory\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124marchive/videos_test\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m    148\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124moutput_directory\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124marchive/videos_testRS\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m    149\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel_file\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124marchive/kinetics400_val_list_videos.txt\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m    150\u001b[0m     }\n\u001b[0;32m--> 152\u001b[0m     \u001b[43mevaluate_staa\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[1], line 117\u001b[0m, in \u001b[0;36mevaluate_staa\u001b[0;34m(config)\u001b[0m\n\u001b[1;32m    114\u001b[0m faithfulness \u001b[38;5;241m=\u001b[39m calculate_faithfulness(extractor, video_path)\n\u001b[1;32m    116\u001b[0m \u001b[38;5;66;03m# Calculate monotonicity using Kendall's Tau\u001b[39;00m\n\u001b[0;32m--> 117\u001b[0m monotonicity \u001b[38;5;241m=\u001b[39m \u001b[43mcalculate_monotonicity\u001b[49m\u001b[43m(\u001b[49m\u001b[43mextractor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvideo_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    119\u001b[0m \u001b[38;5;66;03m# Calculate computation time\u001b[39;00m\n\u001b[1;32m    120\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n",
      "Cell \u001b[0;32mIn[1], line 79\u001b[0m, in \u001b[0;36mcalculate_monotonicity\u001b[0;34m(extractor, video_path)\u001b[0m\n\u001b[1;32m     76\u001b[0m masked_frames \u001b[38;5;241m=\u001b[39m frames\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m     77\u001b[0m masked_frames\u001b[38;5;241m.\u001b[39mflat[top_k_indices] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m---> 79\u001b[0m masked_prediction, _ \u001b[38;5;241m=\u001b[39m \u001b[43mextractor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextract_attention\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmasked_frames\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     80\u001b[0m diff \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mabs(original_prediction \u001b[38;5;241m-\u001b[39m masked_prediction)\u001b[38;5;241m.\u001b[39mmean()\n\u001b[1;32m     81\u001b[0m diffs\u001b[38;5;241m.\u001b[39mappend(diff)\n",
      "Cell \u001b[0;32mIn[1], line 22\u001b[0m, in \u001b[0;36mAttentionExtractor.extract_attention\u001b[0;34m(self, frames)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m frames\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m3\u001b[39m:\n\u001b[1;32m     21\u001b[0m     frames \u001b[38;5;241m=\u001b[39m frames[np\u001b[38;5;241m.\u001b[39mnewaxis, \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m]\n\u001b[0;32m---> 22\u001b[0m inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimage_processor\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mframes\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m     24\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs, output_attentions\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/miniconda3/envs/mmxai/lib/python3.12/site-packages/transformers/image_processing_utils.py:551\u001b[0m, in \u001b[0;36mBaseImageProcessor.__call__\u001b[0;34m(self, images, **kwargs)\u001b[0m\n\u001b[1;32m    549\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, images, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m BatchFeature:\n\u001b[1;32m    550\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Preprocess an image or a batch of images.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpreprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/mmxai/lib/python3.12/site-packages/transformers/models/videomae/image_processing_videomae.py:343\u001b[0m, in \u001b[0;36mVideoMAEImageProcessor.preprocess\u001b[0;34m(self, videos, do_resize, size, resample, do_center_crop, crop_size, do_rescale, rescale_factor, do_normalize, image_mean, image_std, return_tensors, data_format, input_data_format, **kwargs)\u001b[0m\n\u001b[1;32m    334\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    335\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid image type. Must be of type PIL.Image.Image, numpy.ndarray, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    336\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch.Tensor, tf.Tensor or jax.ndarray.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    337\u001b[0m     )\n\u001b[1;32m    339\u001b[0m videos \u001b[38;5;241m=\u001b[39m make_batched(videos)\n\u001b[1;32m    341\u001b[0m videos \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    342\u001b[0m     [\n\u001b[0;32m--> 343\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_preprocess_image\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    344\u001b[0m \u001b[43m            \u001b[49m\u001b[43mimage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    345\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdo_resize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdo_resize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    346\u001b[0m \u001b[43m            \u001b[49m\u001b[43msize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    347\u001b[0m \u001b[43m            \u001b[49m\u001b[43mresample\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresample\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdo_center_crop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdo_center_crop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcrop_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcrop_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    350\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdo_rescale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdo_rescale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrescale_factor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrescale_factor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    352\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdo_normalize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdo_normalize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    353\u001b[0m \u001b[43m            \u001b[49m\u001b[43mimage_mean\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimage_mean\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m            \u001b[49m\u001b[43mimage_std\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimage_std\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdata_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    356\u001b[0m \u001b[43m            \u001b[49m\u001b[43minput_data_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_data_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    357\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    358\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m img \u001b[38;5;129;01min\u001b[39;00m video\n\u001b[1;32m    359\u001b[0m     ]\n\u001b[1;32m    360\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m video \u001b[38;5;129;01min\u001b[39;00m videos\n\u001b[1;32m    361\u001b[0m ]\n\u001b[1;32m    363\u001b[0m data \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpixel_values\u001b[39m\u001b[38;5;124m\"\u001b[39m: videos}\n\u001b[1;32m    364\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m BatchFeature(data\u001b[38;5;241m=\u001b[39mdata, tensor_type\u001b[38;5;241m=\u001b[39mreturn_tensors)\n",
      "File \u001b[0;32m~/miniconda3/envs/mmxai/lib/python3.12/site-packages/transformers/models/videomae/image_processing_videomae.py:238\u001b[0m, in \u001b[0;36mVideoMAEImageProcessor._preprocess_image\u001b[0;34m(self, image, do_resize, size, resample, do_center_crop, crop_size, do_rescale, rescale_factor, do_normalize, image_mean, image_std, data_format, input_data_format)\u001b[0m\n\u001b[1;32m    235\u001b[0m     input_data_format \u001b[38;5;241m=\u001b[39m infer_channel_dimension_format(image)\n\u001b[1;32m    237\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m do_resize:\n\u001b[0;32m--> 238\u001b[0m     image \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresample\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresample\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_data_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_data_format\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    240\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m do_center_crop:\n\u001b[1;32m    241\u001b[0m     image \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcenter_crop(image, size\u001b[38;5;241m=\u001b[39mcrop_size, input_data_format\u001b[38;5;241m=\u001b[39minput_data_format)\n",
      "File \u001b[0;32m~/miniconda3/envs/mmxai/lib/python3.12/site-packages/transformers/models/videomae/image_processing_videomae.py:186\u001b[0m, in \u001b[0;36mVideoMAEImageProcessor.resize\u001b[0;34m(self, image, size, resample, data_format, input_data_format, **kwargs)\u001b[0m\n\u001b[1;32m    184\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSize must have \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mheight\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m and \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwidth\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m or \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mshortest_edge\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m as keys. Got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msize\u001b[38;5;241m.\u001b[39mkeys()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 186\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mresize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    187\u001b[0m \u001b[43m    \u001b[49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    188\u001b[0m \u001b[43m    \u001b[49m\u001b[43msize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    189\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresample\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresample\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    190\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    191\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_data_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_data_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    192\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    193\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/mmxai/lib/python3.12/site-packages/transformers/image_transforms.py:330\u001b[0m, in \u001b[0;36mresize\u001b[0;34m(image, size, resample, reducing_gap, data_format, return_numpy, input_data_format)\u001b[0m\n\u001b[1;32m    328\u001b[0m height, width \u001b[38;5;241m=\u001b[39m size\n\u001b[1;32m    329\u001b[0m \u001b[38;5;66;03m# PIL images are in the format (width, height)\u001b[39;00m\n\u001b[0;32m--> 330\u001b[0m resized_image \u001b[38;5;241m=\u001b[39m \u001b[43mimage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresize\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwidth\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheight\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresample\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresample\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreducing_gap\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreducing_gap\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    332\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_numpy:\n\u001b[1;32m    333\u001b[0m     resized_image \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(resized_image)\n",
      "File \u001b[0;32m~/miniconda3/envs/mmxai/lib/python3.12/site-packages/PIL/Image.py:2200\u001b[0m, in \u001b[0;36mImage.resize\u001b[0;34m(self, size, resample, box, reducing_gap)\u001b[0m\n\u001b[1;32m   2192\u001b[0m             \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mreduce(\u001b[38;5;28mself\u001b[39m, factor, box\u001b[38;5;241m=\u001b[39mreduce_box)\n\u001b[1;32m   2193\u001b[0m         box \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   2194\u001b[0m             (box[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m-\u001b[39m reduce_box[\u001b[38;5;241m0\u001b[39m]) \u001b[38;5;241m/\u001b[39m factor_x,\n\u001b[1;32m   2195\u001b[0m             (box[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m-\u001b[39m reduce_box[\u001b[38;5;241m1\u001b[39m]) \u001b[38;5;241m/\u001b[39m factor_y,\n\u001b[1;32m   2196\u001b[0m             (box[\u001b[38;5;241m2\u001b[39m] \u001b[38;5;241m-\u001b[39m reduce_box[\u001b[38;5;241m0\u001b[39m]) \u001b[38;5;241m/\u001b[39m factor_x,\n\u001b[1;32m   2197\u001b[0m             (box[\u001b[38;5;241m3\u001b[39m] \u001b[38;5;241m-\u001b[39m reduce_box[\u001b[38;5;241m1\u001b[39m]) \u001b[38;5;241m/\u001b[39m factor_y,\n\u001b[1;32m   2198\u001b[0m         )\n\u001b[0;32m-> 2200\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_new(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mim\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresize\u001b[49m\u001b[43m(\u001b[49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresample\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbox\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import av\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import os\n",
    "from transformers import TimesformerForVideoClassification, AutoImageProcessor\n",
    "from scipy.stats import kendalltau\n",
    "import json\n",
    "\n",
    "class AttentionExtractor:\n",
    "    def __init__(self, model_name, device='cuda' if torch.cuda.is_available() else 'cpu'):\n",
    "        self.model = TimesformerForVideoClassification.from_pretrained(model_name)\n",
    "        self.model.to(device)\n",
    "        self.device = device\n",
    "        self.image_processor = AutoImageProcessor.from_pretrained(model_name)\n",
    "\n",
    "    def extract_attention(self, frames):\n",
    "        # 确保帧数据的形状正确\n",
    "        if frames.ndim == 3:\n",
    "            frames = frames[np.newaxis, ...]\n",
    "        inputs = self.image_processor(list(frames), return_tensors=\"pt\").to(self.device)\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(**inputs, output_attentions=True)\n",
    "        last_layer_attention = outputs.attentions[-1]\n",
    "        spatial_attention = last_layer_attention.mean(1)\n",
    "        return outputs.logits.cpu().numpy(), spatial_attention.cpu().numpy()\n",
    "\n",
    "def load_video(video_path, num_frames=8):\n",
    "    container = av.open(video_path)\n",
    "    video_stream = container.streams.video[0]\n",
    "    \n",
    "    frames = []\n",
    "    for frame in container.decode(video=0):\n",
    "        frames.append(frame.to_rgb().to_ndarray())\n",
    "        if len(frames) == num_frames:\n",
    "            break\n",
    "    \n",
    "    # 如果视频帧数少于 num_frames，则重复最后一帧\n",
    "    while len(frames) < num_frames:\n",
    "        frames.append(frames[-1])\n",
    "    \n",
    "    # 确保帧数组的形状正确\n",
    "    frames = np.stack(frames)\n",
    "    \n",
    "    return frames\n",
    "\n",
    "def calculate_faithfulness(extractor, video_path):\n",
    "    frames = load_video(video_path)\n",
    "    original_prediction, attention = extractor.extract_attention(frames)\n",
    "    \n",
    "    flat_attention = attention.flatten()\n",
    "    k = int(0.5 * len(flat_attention))\n",
    "    top_k_indices = np.argsort(flat_attention)[-k:]\n",
    "    \n",
    "    masked_frames = frames.copy()\n",
    "    masked_frames.flat[top_k_indices] = 0\n",
    "    \n",
    "    masked_prediction, _ = extractor.extract_attention(masked_frames)\n",
    "    \n",
    "    faithfulness = 1 - np.abs(original_prediction - masked_prediction).mean()\n",
    "    return faithfulness\n",
    "\n",
    "def calculate_monotonicity(extractor, video_path):\n",
    "    frames = load_video(video_path)\n",
    "    original_prediction, attention = extractor.extract_attention(frames)\n",
    "    \n",
    "    flat_attention = attention.flatten()\n",
    "    percentages = np.arange(0.1, 1.0, 0.1)\n",
    "    diffs = []\n",
    "    \n",
    "    for p in percentages:\n",
    "        k = int(p * len(flat_attention))\n",
    "        top_k_indices = np.argsort(flat_attention)[-k:]\n",
    "        \n",
    "        masked_frames = frames.copy()\n",
    "        masked_frames.flat[top_k_indices] = 0\n",
    "        \n",
    "        masked_prediction, _ = extractor.extract_attention(masked_frames)\n",
    "        diff = np.abs(original_prediction - masked_prediction).mean()\n",
    "        diffs.append(diff)\n",
    "    \n",
    "    tau, _ = kendalltau(percentages, diffs)\n",
    "    return tau\n",
    "\n",
    "def load_video_labels(label_file):\n",
    "    video_labels = {}\n",
    "    with open(label_file, 'r') as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split()\n",
    "            if len(parts) == 2:\n",
    "                video_name, label = parts\n",
    "                video_labels[video_name.split('.')[0]] = int(label)\n",
    "    return video_labels\n",
    "\n",
    "def evaluate_staa(config):\n",
    "    extractor = AttentionExtractor(config['model_name'])\n",
    "    video_labels = load_video_labels(config['label_file'])\n",
    "    \n",
    "    results = {}\n",
    "    video_files = [f for f in os.listdir(config['video_directory']) if f.endswith('.mp4')]\n",
    "    \n",
    "    for video_file in tqdm(video_files, desc=\"Processing videos\"):\n",
    "        video_name = video_file.split('.')[0]\n",
    "        video_path = os.path.join(config['video_directory'], video_file)\n",
    "        \n",
    "        if video_name not in video_labels:\n",
    "            print(f\"Warning: No label found for {video_name}. Skipping.\")\n",
    "            continue\n",
    "        \n",
    "        true_label = video_labels[video_name]\n",
    "        \n",
    "        # Calculate faithfulness\n",
    "        faithfulness = calculate_faithfulness(extractor, video_path)\n",
    "        \n",
    "        # Calculate monotonicity using Kendall's Tau\n",
    "        monotonicity = calculate_monotonicity(extractor, video_path)\n",
    "        \n",
    "        # Calculate computation time\n",
    "        start_time = time.time()\n",
    "        _ = extractor.extract_attention(load_video(video_path))\n",
    "        computation_time = time.time() - start_time\n",
    "        \n",
    "        results[video_name] = {\n",
    "            \"true_label\": true_label,\n",
    "            \"faithfulness\": faithfulness,\n",
    "            \"monotonicity\": monotonicity,\n",
    "            \"computation_time\": computation_time\n",
    "        }\n",
    "    \n",
    "    # Calculate overall statistics\n",
    "    faithfulness_scores = [r['faithfulness'] for r in results.values()]\n",
    "    monotonicity_scores = [r['monotonicity'] for r in results.values()]\n",
    "    computation_times = [r['computation_time'] for r in results.values()]\n",
    "    \n",
    "    print(f\"Overall Faithfulness: {np.mean(faithfulness_scores):.2f} ± {np.std(faithfulness_scores):.2f}\")\n",
    "    print(f\"Overall Monotonicity (Kendall's Tau): {np.mean(monotonicity_scores):.2f} ± {np.std(monotonicity_scores):.2f}\")\n",
    "    print(f\"Average Computation Time: {np.mean(computation_times):.2f} seconds\")\n",
    "    \n",
    "    # Save results to a JSON file\n",
    "    with open(os.path.join(config['output_directory'], 'evaluation_results.json'), 'w') as f:\n",
    "        json.dump(results, f, indent=4)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    config = {\n",
    "        'model_name': 'facebook/timesformer-base-finetuned-k400',\n",
    "        'video_directory': 'archive/videos_test',\n",
    "        'output_directory': 'archive/videos_testRS',\n",
    "        'label_file': 'archive/kinetics400_val_list_videos.txt',\n",
    "    }\n",
    "    \n",
    "    evaluate_staa(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "修改STAA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing videos:   0%|          | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frames shape: (8, 320, 570, 3)\n",
      "Attention shape: (8, 197, 197)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing videos:  25%|██▌       | 1/4 [00:02<00:07,  2.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frames shape: (8, 320, 426, 3)\n",
      "Attention shape: (8, 197, 197)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing videos:  50%|█████     | 2/4 [00:04<00:04,  2.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frames shape: (8, 320, 426, 3)\n",
      "Attention shape: (8, 197, 197)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing videos:  75%|███████▌  | 3/4 [00:06<00:02,  2.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frames shape: (8, 320, 426, 3)\n",
      "Attention shape: (8, 197, 197)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing videos: 100%|██████████| 4/4 [00:09<00:00,  2.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall Faithfulness: -1.15 ± 0.15\n",
      "Overall Monotonicity (Kendall's Tau): -0.89 ± 0.04\n",
      "Average Computation Time: 0.16 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import av\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import os\n",
    "from transformers import TimesformerForVideoClassification, AutoImageProcessor\n",
    "from scipy.stats import kendalltau\n",
    "import json\n",
    "\n",
    "class AttentionExtractor:\n",
    "    def __init__(self, model_name, device='cuda' if torch.cuda.is_available() else 'cpu'):\n",
    "        self.model = TimesformerForVideoClassification.from_pretrained(model_name)\n",
    "        self.model.to(device)\n",
    "        self.device = device\n",
    "        self.image_processor = AutoImageProcessor.from_pretrained(model_name)\n",
    "\n",
    "    def extract_attention(self, frames):\n",
    "        # 确保帧数据的形状正确\n",
    "        if frames.ndim == 3:\n",
    "            frames = frames[np.newaxis, ...]\n",
    "        inputs = self.image_processor(list(frames), return_tensors=\"pt\").to(self.device)\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(**inputs, output_attentions=True)\n",
    "        last_layer_attention = outputs.attentions[-1]\n",
    "        spatial_attention = last_layer_attention.mean(1)\n",
    "        return outputs.logits.cpu().numpy(), spatial_attention.cpu().numpy()\n",
    "\n",
    "def load_video(video_path, num_frames=8):\n",
    "    container = av.open(video_path)\n",
    "    video_stream = container.streams.video[0]\n",
    "    \n",
    "    frames = []\n",
    "    for frame in container.decode(video=0):\n",
    "        frames.append(frame.to_rgb().to_ndarray())\n",
    "        if len(frames) == num_frames:\n",
    "            break\n",
    "    \n",
    "    # 如果视频帧数少于 num_frames，则重复最后一帧\n",
    "    while len(frames) < num_frames:\n",
    "        frames.append(frames[-1])\n",
    "    \n",
    "    # 确保帧数组的形状正确\n",
    "    frames = np.stack(frames)\n",
    "    \n",
    "    return frames\n",
    "\n",
    "\n",
    "def calculate_faithfulness(extractor, video_path):\n",
    "    frames = load_video(video_path)\n",
    "    original_prediction, attention = extractor.extract_attention(frames)\n",
    "    \n",
    "    print(\"Frames shape:\", frames.shape)\n",
    "    print(\"Attention shape:\", attention.shape)\n",
    "    \n",
    "    # 获取帧的形状\n",
    "    frame_shape = frames.shape\n",
    "    attention_shape = attention.shape\n",
    "    \n",
    "    # 计算要保留的像素数量\n",
    "    k = int(0.5 * attention.size)\n",
    "    \n",
    "    # 获取最重要的50%的索引\n",
    "    flat_attention = attention.flatten()\n",
    "    top_k_indices = np.argsort(flat_attention)[-k:]\n",
    "    \n",
    "    # 创建掩码\n",
    "    mask = np.zeros_like(flat_attention)\n",
    "    mask[top_k_indices] = 1\n",
    "    mask = mask.reshape(attention_shape)\n",
    "    \n",
    "    # 将mask调整为与帧相同的大小\n",
    "    mask_resized = np.zeros(frame_shape[:-1])  # 不包括颜色通道\n",
    "    for i in range(frame_shape[0]):  # 对每一帧\n",
    "        mask_resized[i] = cv2.resize(mask[i], (frame_shape[2], frame_shape[1]))\n",
    "    \n",
    "    # 应用掩码到原始帧\n",
    "    masked_frames = frames.copy()\n",
    "    for i in range(frame_shape[-1]):  # 对每个颜色通道应用掩码\n",
    "        masked_frames[..., i] = frames[..., i] * mask_resized\n",
    "    \n",
    "    masked_prediction, _ = extractor.extract_attention(masked_frames)\n",
    "    \n",
    "    # 计算faithfulness\n",
    "    faithfulness = 1 - np.abs(original_prediction - masked_prediction).mean()\n",
    "    return faithfulness\n",
    "\n",
    "def calculate_monotonicity(extractor, video_path):\n",
    "    frames = load_video(video_path)\n",
    "    original_prediction, attention = extractor.extract_attention(frames)\n",
    "    \n",
    "    frame_shape = frames.shape\n",
    "    attention_shape = attention.shape\n",
    "    \n",
    "    flat_attention = attention.flatten()\n",
    "    percentages = np.arange(0.1, 1.0, 0.1)\n",
    "    diffs = []\n",
    "    \n",
    "    for p in percentages:\n",
    "        k = int(p * flat_attention.size)\n",
    "        top_k_indices = np.argsort(flat_attention)[-k:]\n",
    "        \n",
    "        # 创建掩码\n",
    "        mask = np.zeros_like(flat_attention)\n",
    "        mask[top_k_indices] = 1\n",
    "        mask = mask.reshape(attention_shape)\n",
    "        \n",
    "        # 将mask调整为与帧相同的大小\n",
    "        mask_resized = np.zeros(frame_shape[:-1])  # 不包括颜色通道\n",
    "        for i in range(frame_shape[0]):  # 对每一帧\n",
    "            mask_resized[i] = cv2.resize(mask[i], (frame_shape[2], frame_shape[1]))\n",
    "        \n",
    "        # 应用掩码到原始帧\n",
    "        masked_frames = frames.copy()\n",
    "        for i in range(frame_shape[-1]):  # 对每个颜色通道应用掩码\n",
    "            masked_frames[..., i] = frames[..., i] * mask_resized\n",
    "        \n",
    "        masked_prediction, _ = extractor.extract_attention(masked_frames)\n",
    "        diff = np.abs(original_prediction - masked_prediction).mean()\n",
    "        diffs.append(diff)\n",
    "    \n",
    "    tau, _ = kendalltau(percentages, diffs)\n",
    "    return tau\n",
    "\n",
    "def load_video_labels(label_file):\n",
    "    video_labels = {}\n",
    "    with open(label_file, 'r') as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split()\n",
    "            if len(parts) == 2:\n",
    "                video_name, label = parts\n",
    "                video_labels[video_name.split('.')[0]] = int(label)\n",
    "    return video_labels\n",
    "\n",
    "def evaluate_staa(config):\n",
    "    extractor = AttentionExtractor(config['model_name'])\n",
    "    video_labels = load_video_labels(config['label_file'])\n",
    "    \n",
    "    results = {}\n",
    "    video_files = [f for f in os.listdir(config['video_directory']) if f.endswith('.mp4')]\n",
    "    \n",
    "    for video_file in tqdm(video_files, desc=\"Processing videos\"):\n",
    "        video_name = video_file.split('.')[0]\n",
    "        video_path = os.path.join(config['video_directory'], video_file)\n",
    "        \n",
    "        if video_name not in video_labels:\n",
    "            print(f\"Warning: No label found for {video_name}. Skipping.\")\n",
    "            continue\n",
    "        \n",
    "        true_label = video_labels[video_name]\n",
    "        \n",
    "        # Calculate faithfulness\n",
    "        faithfulness = calculate_faithfulness(extractor, video_path)\n",
    "        \n",
    "        # Calculate monotonicity using Kendall's Tau\n",
    "        monotonicity = calculate_monotonicity(extractor, video_path)\n",
    "        \n",
    "        # Calculate computation time\n",
    "        start_time = time.time()\n",
    "        _ = extractor.extract_attention(load_video(video_path))\n",
    "        computation_time = time.time() - start_time\n",
    "        \n",
    "        results[video_name] = {\n",
    "            \"true_label\": true_label,\n",
    "            \"faithfulness\": faithfulness,\n",
    "            \"monotonicity\": monotonicity,\n",
    "            \"computation_time\": computation_time\n",
    "        }\n",
    "    \n",
    "    # Calculate overall statistics\n",
    "    faithfulness_scores = [r['faithfulness'] for r in results.values()]\n",
    "    monotonicity_scores = [r['monotonicity'] for r in results.values()]\n",
    "    computation_times = [r['computation_time'] for r in results.values()]\n",
    "    \n",
    "    print(f\"Overall Faithfulness: {np.mean(faithfulness_scores):.2f} ± {np.std(faithfulness_scores):.2f}\")\n",
    "    print(f\"Overall Monotonicity (Kendall's Tau): {np.mean(monotonicity_scores):.2f} ± {np.std(monotonicity_scores):.2f}\")\n",
    "    print(f\"Average Computation Time: {np.mean(computation_times):.2f} seconds\")\n",
    "    \n",
    "    # Save results to a JSON file\n",
    "    with open(os.path.join(config['output_directory'], 'evaluation_results.json'), 'w') as f:\n",
    "        json.dump(results, f, indent=4)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    config = {\n",
    "        'model_name': 'facebook/timesformer-base-finetuned-k400',\n",
    "        'video_directory': 'archive/videos_test',\n",
    "        'output_directory': 'archive/videos_testRS2',\n",
    "        'label_file': 'archive/kinetics400_val_list_videos.txt',\n",
    "    }\n",
    "    \n",
    "    evaluate_staa(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SHAP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "保留重要的段"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration. Please open a PR/issue to update `preprocessor_config.json` to use `image_processor_type` instead of `feature_extractor_type`. This warning will be removed in v4.40.\n",
      "Evaluating videos: 100%|██████████| 10/10 [00:57<00:00,  5.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faithfulness: 0.8515 ± 0.1543\n",
      "Monotonicity: -0.5353 ± 0.0000\n",
      "Computation Time: 5.74 ± 0.13 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import av\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoImageProcessor, TimesformerForVideoClassification\n",
    "from tqdm import tqdm\n",
    "from scipy.stats import kendalltau\n",
    "import time\n",
    "import json\n",
    "import random\n",
    "from collections import defaultdict\n",
    "from itertools import combinations \n",
    "\n",
    "class VideoProcessor:\n",
    "    def __init__(self, model_name, image_processor_name, device='cuda'):\n",
    "        self.model = TimesformerForVideoClassification.from_pretrained(model_name)\n",
    "        self.image_processor = AutoImageProcessor.from_pretrained(image_processor_name)\n",
    "        self.device = device\n",
    "        self.model.to(device)\n",
    "\n",
    "    def split_video_into_segments(self, container, n_segments=8, frames_per_segment=16):\n",
    "        frame_list = [frame.to_image() for frame in container.decode(video=0)]\n",
    "        total_frames = len(frame_list)\n",
    "        segment_length = total_frames // n_segments\n",
    "        segments = []\n",
    "        for i in range(n_segments):\n",
    "            start = i * segment_length\n",
    "            end = min(start + segment_length, total_frames)\n",
    "            segment_frames = frame_list[start:end] if end - start == segment_length else frame_list[start:] + [frame_list[-1]] * (segment_length - (end - start))\n",
    "            segments.append(segment_frames[:frames_per_segment])\n",
    "        return segments\n",
    "\n",
    "    def predict_video_and_segments(self, container, true_label):\n",
    "        video_segments = self.split_video_into_segments(container)\n",
    "        segment_outputs = []\n",
    "        with torch.no_grad():\n",
    "            for segment in video_segments:\n",
    "                inputs = self.image_processor(list(segment), return_tensors=\"pt\")\n",
    "                inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
    "                try:\n",
    "                    outputs = self.model(**inputs)\n",
    "                    logits = outputs.logits\n",
    "                    probabilities = F.softmax(logits, dim=-1)\n",
    "                    pred_label = logits.argmax(-1).item()\n",
    "                    pred_score = probabilities[0, pred_label].item()\n",
    "                    segment_outputs.append((pred_label, pred_score, probabilities))\n",
    "                except RuntimeError as e:\n",
    "                    print(f\"Error processing segment: {e}\")\n",
    "                    continue\n",
    "        return segment_outputs\n",
    "\n",
    "class TemporalShap:\n",
    "    def __init__(self, num_samples=100):\n",
    "        self.num_samples = num_samples\n",
    "\n",
    "    def exact_shapley_values(self, segment_outputs, label_index):\n",
    "        n = len(segment_outputs)\n",
    "        shapley_values = [0] * n\n",
    "        all_indices = list(range(n))\n",
    "        for i in all_indices:\n",
    "            marginal_contributions = []\n",
    "            for subset_size in range(n):\n",
    "                subsets = list(combinations([x for x in all_indices if x != i], subset_size))\n",
    "                for subset in subsets:\n",
    "                    subset_prob = torch.zeros_like(segment_outputs[0][2])\n",
    "                    if subset:\n",
    "                        subset_prob = torch.mean(torch.stack([segment_outputs[j][2] for j in subset]), dim=0)\n",
    "                    with_i_prob = (subset_prob * len(subset) + segment_outputs[i][2]) / (len(subset) + 1)\n",
    "                    marginal_contributions.append(with_i_prob[0, label_index].item() - subset_prob[0, label_index].item())\n",
    "            shapley_values[i] = np.mean(marginal_contributions)\n",
    "        return shapley_values\n",
    "\n",
    "class ShapEvaluator:\n",
    "    def __init__(self, video_processor, shap_calculator):\n",
    "        self.video_processor = video_processor\n",
    "        self.shap_calculator = shap_calculator\n",
    "\n",
    "    def calculate_faithfulness(self, video_path, true_label):\n",
    "        container = av.open(video_path)\n",
    "        segment_outputs = self.video_processor.predict_video_and_segments(container, true_label)\n",
    "        if not segment_outputs:\n",
    "            return None\n",
    "\n",
    "        video_probs = torch.mean(torch.stack([output[2] for output in segment_outputs]), dim=0)\n",
    "        original_prediction = video_probs[0, true_label].item()\n",
    "\n",
    "        shapley_values = self.shap_calculator.exact_shapley_values(segment_outputs, true_label)\n",
    "        \n",
    "        # 选择 Shapley 值最高的 50% 的段（最重要的段）\n",
    "        k = len(shapley_values) // 2\n",
    "        most_important_indices = np.argsort(shapley_values)[-k:]\n",
    "        \n",
    "        # 只保留重要的段\n",
    "        important_probs = [output[2] for i, output in enumerate(segment_outputs) if i in most_important_indices]\n",
    "        \n",
    "        if important_probs:\n",
    "            important_video_probs = torch.mean(torch.stack(important_probs), dim=0)\n",
    "            important_prediction = important_video_probs[0, true_label].item()\n",
    "        else:\n",
    "            # 如果没有重要的段，则假设预测是随机的\n",
    "            important_prediction = 1.0 / video_probs.shape[1]  # 假设是均匀分布\n",
    "\n",
    "        faithfulness = 1 - abs(original_prediction - important_prediction)\n",
    "        return faithfulness\n",
    "\n",
    "    def calculate_monotonicity(self, video_path, true_label):\n",
    "        container = av.open(video_path)\n",
    "        segment_outputs = self.video_processor.predict_video_and_segments(container, true_label)\n",
    "        if not segment_outputs:\n",
    "            return None\n",
    "\n",
    "        shapley_values = self.shap_calculator.exact_shapley_values(segment_outputs, true_label)\n",
    "        \n",
    "        percentages = np.arange(0.1, 1.0, 0.1)\n",
    "        diffs = []\n",
    "\n",
    "        original_probs = torch.mean(torch.stack([output[2] for output in segment_outputs]), dim=0)\n",
    "        original_prediction = original_probs[0, true_label].item()\n",
    "\n",
    "        for p in percentages:\n",
    "            k = int(p * len(shapley_values))\n",
    "            most_important_indices = np.argsort(shapley_values)[-k:]\n",
    "            \n",
    "            important_probs = [output[2] for i, output in enumerate(segment_outputs) if i in most_important_indices]\n",
    "            \n",
    "            if important_probs:\n",
    "                important_video_probs = torch.mean(torch.stack(important_probs), dim=0)\n",
    "                important_prediction = important_video_probs[0, true_label].item()\n",
    "            else:\n",
    "                important_prediction = 1.0 / original_probs.shape[1]  # 假设是均匀分布\n",
    "            \n",
    "            diff = abs(original_prediction - important_prediction)\n",
    "            diffs.append(diff)\n",
    "\n",
    "        tau, _ = kendalltau(percentages, diffs)\n",
    "        return tau\n",
    "\n",
    "def load_video_labels(label_file):\n",
    "    video_labels = {}\n",
    "    with open(label_file, 'r') as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split()\n",
    "            if len(parts) == 2:\n",
    "                video_name, label = parts\n",
    "                video_labels[video_name.split('.')[0]] = int(label)\n",
    "    return video_labels\n",
    "\n",
    "def evaluate_shap(config):\n",
    "    video_processor = VideoProcessor(config[\"model_name\"], config[\"image_processor_name\"])\n",
    "    shap_calculator = TemporalShap(num_samples=config[\"num_samples\"])\n",
    "    evaluator = ShapEvaluator(video_processor, shap_calculator)\n",
    "\n",
    "    video_labels = load_video_labels(config[\"video_list_path\"])\n",
    "    \n",
    "    faithfulness_scores = []\n",
    "    monotonicity_scores = []\n",
    "    computation_times = []\n",
    "\n",
    "    for video_name, true_label in tqdm(list(video_labels.items())[:config[\"num_eval_videos\"]], desc=\"Evaluating videos\"):\n",
    "        video_path = os.path.join(config[\"video_directory\"], video_name + '.mp4')\n",
    "        \n",
    "        start_time = time.time()\n",
    "        \n",
    "        faithfulness = evaluator.calculate_faithfulness(video_path, true_label)\n",
    "        monotonicity = evaluator.calculate_monotonicity(video_path, true_label)\n",
    "        \n",
    "        end_time = time.time()\n",
    "        computation_time = end_time - start_time\n",
    "\n",
    "        if faithfulness is not None and monotonicity is not None:\n",
    "            faithfulness_scores.append(faithfulness)\n",
    "            monotonicity_scores.append(monotonicity)\n",
    "            computation_times.append(computation_time)\n",
    "\n",
    "    results = {\n",
    "        \"faithfulness\": {\n",
    "            \"mean\": np.mean(faithfulness_scores),\n",
    "            \"std\": np.std(faithfulness_scores)\n",
    "        },\n",
    "        \"monotonicity\": {\n",
    "            \"mean\": np.mean(monotonicity_scores),\n",
    "            \"std\": np.std(monotonicity_scores)\n",
    "        },\n",
    "        \"computation_time\": {\n",
    "            \"mean\": np.mean(computation_times),\n",
    "            \"std\": np.std(computation_times)\n",
    "        }\n",
    "    }\n",
    "\n",
    "    with open(\"shap_evaluation_results.json\", \"w\") as f:\n",
    "        json.dump(results, f, indent=4)\n",
    "\n",
    "    print(f\"Faithfulness: {results['faithfulness']['mean']:.4f} ± {results['faithfulness']['std']:.4f}\")\n",
    "    print(f\"Monotonicity: {results['monotonicity']['mean']:.4f} ± {results['monotonicity']['std']:.4f}\")\n",
    "    print(f\"Computation Time: {results['computation_time']['mean']:.2f} ± {results['computation_time']['std']:.2f} seconds\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    config = {\n",
    "        \"model_name\": \"facebook/timesformer-base-finetuned-k400\",\n",
    "        \"image_processor_name\": \"MCG-NJU/videomae-base-finetuned-kinetics\",\n",
    "        \"num_samples\": 100,\n",
    "        \"video_list_path\": \"archive/kinetics400_val_list_videos.txt\",\n",
    "        \"video_directory\": \"archive/videos_test\",\n",
    "        \"num_eval_videos\": 1200  # 设置要评估的视频数量\n",
    "    }\n",
    "    \n",
    "    evaluate_shap(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LIME\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-02 17:07:55,374 - INFO - Using device: cuda\n",
      "/home/z/miniconda3/envs/mmxai/lib/python3.12/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/home/z/miniconda3/envs/mmxai/lib/python3.12/site-packages/transformers/models/vit/feature_extraction_vit.py:28: FutureWarning: The class ViTFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use ViTImageProcessor instead.\n",
      "  warnings.warn(\n",
      "2024-10-02 17:07:56,530 - INFO - Model and feature extractor loaded successfully\n",
      "Evaluating videos:   0%|          | 0/4 [00:00<?, ?it/s]2024-10-02 17:07:56,541 - INFO - Processing video: archive/example/-8oPwToqArE.mp4 (Label: 265)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32794e78a6004d3d91414bfad1c9d6a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-02 17:08:09,864 - WARNING - All difference values are the same, monotonicity is undefined\n",
      "2024-10-02 17:08:09,865 - WARNING - Unable to calculate monotonicity for frame 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f6c6f17aaa54405acfb46133b186af5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-02 17:08:23,152 - WARNING - All difference values are the same, monotonicity is undefined\n",
      "2024-10-02 17:08:23,153 - WARNING - Unable to calculate monotonicity for frame 2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de6abb5a584c410b88db80788b38ecb9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46e30d235b114878aba1e10992506046",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-02 17:08:49,269 - WARNING - All difference values are the same, monotonicity is undefined\n",
      "2024-10-02 17:08:49,270 - WARNING - Unable to calculate monotonicity for frame 4\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87dad680592f488ea4ac77f6393951e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-02 17:09:02,458 - WARNING - All difference values are the same, monotonicity is undefined\n",
      "2024-10-02 17:09:02,459 - WARNING - Unable to calculate monotonicity for frame 5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48eef3f042254e949ae43ab702c524dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-02 17:09:15,573 - WARNING - All difference values are the same, monotonicity is undefined\n",
      "2024-10-02 17:09:15,573 - WARNING - Unable to calculate monotonicity for frame 6\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ab9e26e83d048b68479f2fc1dc5ddfc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02d321d06d37487a950ee5f4e5d65689",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-02 17:09:41,376 - WARNING - All difference values are the same, monotonicity is undefined\n",
      "2024-10-02 17:09:41,376 - WARNING - Unable to calculate monotonicity for frame 8\n",
      "2024-10-02 17:09:41,377 - INFO - Processed -8oPwToqArE.mp4\n",
      "2024-10-02 17:09:41,377 - INFO -   Faithfulness: 0.0182 ± 0.0079\n",
      "2024-10-02 17:09:41,377 - INFO -   Monotonicity: -0.4472 ± 0.0000\n",
      "Evaluating videos:  25%|██▌       | 1/4 [01:44<05:14, 104.84s/it]2024-10-02 17:09:41,378 - INFO - Processing video: archive/example/-6wNVod8iag.mp4 (Label: 241)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f735500c8cc945838169c04b2508d1fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "318569eead674ccb9ee7973b9ea61e51",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06fd10a0360345569d76a01f3c34bc73",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "acda94fe52d3457e819db1333fa8a550",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aff042d7cec441a18de9e07a2e239af1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b75f51dbf2548eb948dfc5764914e04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7438fbf2394e4a57a0f4c33097ccf27e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-02 17:10:50,653 - WARNING - All difference values are the same, monotonicity is undefined\n",
      "2024-10-02 17:10:50,654 - WARNING - Unable to calculate monotonicity for frame 7\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45f1595fc745462f8091f10bed778bf7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-02 17:11:00,496 - INFO - Processed -6wNVod8iag.mp4\n",
      "2024-10-02 17:11:00,497 - INFO -   Faithfulness: 0.2128 ± 0.0017\n",
      "2024-10-02 17:11:00,497 - INFO -   Monotonicity: -0.4472 ± 0.0000\n",
      "Evaluating videos:  50%|█████     | 2/4 [03:03<02:59, 89.71s/it] 2024-10-02 17:11:00,498 - INFO - Processing video: archive/example/-9i4bm2OiZ4.mp4 (Label: 201)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ceaaad2c449945b0adface34ed365d77",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e743385118154179a0b7d67b1b7dc7a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c5e2b074f794501a1c846a713fc96ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f8605762a014a1c9e390b706e23c894",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf5b2588c89f4b9f9381a28aa1458de7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d8d97a399c44f8093f90740187b3d24",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92c23c163e0848fa9d384817b67034e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d613219435740dc94296ddde31255ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-02 17:12:18,495 - INFO - Processed -9i4bm2OiZ4.mp4\n",
      "2024-10-02 17:12:18,496 - INFO -   Faithfulness: 0.9589 ± 0.0051\n",
      "2024-10-02 17:12:18,496 - INFO -   Monotonicity: -0.4472 ± 0.0000\n",
      "Evaluating videos:  75%|███████▌  | 3/4 [04:21<01:24, 84.36s/it]2024-10-02 17:12:18,497 - INFO - Processing video: archive/example/-0ew-c0w7uc.mp4 (Label: 122)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cfe899c9675940b992c31bfcc461a9a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4cda8c7cec440c286a9432a6345b7a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9cb5d4236d3c4da796bffe3119b1f219",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "529880b019eb48fdbf3482dba0a4eb0a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8fdacfa32ecc4da3ad4cabc637839155",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76de2ec4f05d4444bd3d536331719209",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "324b0a8702c04bcea2dbeb051d70a7f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94a96bbd04704810b0700dc72423ff1a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-02 17:13:35,081 - INFO - Processed -0ew-c0w7uc.mp4\n",
      "2024-10-02 17:13:35,083 - INFO -   Faithfulness: 0.3956 ± 0.0063\n",
      "2024-10-02 17:13:35,083 - INFO -   Monotonicity: -0.5031 ± 0.0722\n",
      "Evaluating videos: 100%|██████████| 4/4 [05:38<00:00, 84.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faithfulness: 0.3964 ± 0.3512\n",
      "Monotonicity: -0.4651 ± 0.0484\n",
      "Computation Time: 84.63 ± 11.70 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import av\n",
    "import torch\n",
    "from transformers import ViTForImageClassification, ViTFeatureExtractor, ViTConfig\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from lime import lime_image\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import logging\n",
    "from torch.cuda.amp import autocast\n",
    "from scipy.stats import kendalltau\n",
    "import time\n",
    "\n",
    "# 设置日志\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# Configuration\n",
    "config = {\n",
    "    \"model_config\": \"google/vit-base-patch16-224\",\n",
    "    \"model_path\": \"finetuned_vit_model_20.pth\",\n",
    "    \"feature_extractor_name\": \"google/vit-base-patch16-224\",\n",
    "    \"video_directory\": \"archive/videos_test\",\n",
    "    \"results_folder\": \"archive/videos_testRSLIME\",\n",
    "    \"num_classes\": 400,\n",
    "    \"num_frames_per_video\": 8,\n",
    "    \"lime_num_samples\": 300,\n",
    "    \"video_list_path\": \"archive/kinetics400_val_list_videos.txt\",\n",
    "    \"num_eval_videos\": 1200  # 设置要评估的视频数量，如果想处理所有视频，可以设置为一个很大的数\n",
    "}\n",
    "\n",
    "# 确保结果目录存在\n",
    "os.makedirs(config[\"results_folder\"], exist_ok=True)\n",
    "\n",
    "# 设置设备\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "logging.info(f\"Using device: {device}\")\n",
    "\n",
    "# 加载模型和特征提取器\n",
    "try:\n",
    "    model_config = ViTConfig.from_pretrained(config[\"model_config\"], num_labels=config[\"num_classes\"])\n",
    "    model = ViTForImageClassification(model_config)\n",
    "    model.load_state_dict(torch.load(config[\"model_path\"], map_location=device))\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    feature_extractor = ViTFeatureExtractor.from_pretrained(config[\"feature_extractor_name\"])\n",
    "    logging.info(\"Model and feature extractor loaded successfully\")\n",
    "except Exception as e:\n",
    "    logging.error(f\"Error loading model or feature extractor: {e}\")\n",
    "    raise\n",
    "\n",
    "def load_video_labels(label_file):\n",
    "    video_labels = {}\n",
    "    with open(label_file, 'r') as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split()\n",
    "            if len(parts) == 2:\n",
    "                video_name, label = parts\n",
    "                video_labels[video_name.split('.')[0]] = int(label)\n",
    "    return video_labels\n",
    "\n",
    "def extract_frames(video_path, num_frames):\n",
    "    frames = []\n",
    "    try:\n",
    "        with av.open(video_path) as container:\n",
    "            stream = container.streams.video[0]\n",
    "            duration = stream.duration * stream.time_base\n",
    "            for i in range(num_frames):\n",
    "                target_ts = duration * (i + 1) / (num_frames + 1)\n",
    "                container.seek(int(target_ts / stream.time_base))\n",
    "                for frame in container.decode(video=0):\n",
    "                    frames.append(frame.to_image())\n",
    "                    break\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error extracting frames from {video_path}: {e}\")\n",
    "    return frames\n",
    "\n",
    "def calculate_faithfulness(original_prediction, masked_prediction):\n",
    "    return 1 - abs(original_prediction - masked_prediction)\n",
    "\n",
    "def calculate_monotonicity(percentages, diffs):\n",
    "    if len(percentages) != len(diffs):\n",
    "        logging.warning(f\"Mismatch in lengths: percentages ({len(percentages)}) and diffs ({len(diffs)})\")\n",
    "        return None\n",
    "    \n",
    "    if len(set(diffs)) == 1:  # 如果所有的差异值都相同\n",
    "        logging.warning(\"All difference values are the same, monotonicity is undefined\")\n",
    "        return None\n",
    "    \n",
    "    if np.isnan(diffs).any() or np.isinf(diffs).any():\n",
    "        logging.warning(\"NaN or Inf values found in diffs\")\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        tau, p_value = kendalltau(percentages, diffs)\n",
    "        if np.isnan(tau):\n",
    "            logging.warning(\"Kendall's tau is NaN\")\n",
    "            return None\n",
    "        return tau\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error in calculating Kendall's tau: {e}\")\n",
    "        return None\n",
    "\n",
    "def process_and_explain(video_path, model, feature_extractor, num_frames):\n",
    "    frames = extract_frames(video_path, num_frames)\n",
    "    if not frames:\n",
    "        logging.error(f\"No frames extracted from {video_path}\")\n",
    "        return None\n",
    "\n",
    "    results = []\n",
    "    for i, frame in enumerate(frames):\n",
    "        try:\n",
    "            inputs = feature_extractor(images=frame, return_tensors=\"pt\").to(device)\n",
    "            with autocast():\n",
    "                with torch.no_grad():\n",
    "                    outputs = model(**inputs)\n",
    "            preds = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "            top_pred = preds.argmax().item()\n",
    "            original_prediction = preds[0, top_pred].item()\n",
    "\n",
    "            def batch_predict(images):\n",
    "                batch_inputs = feature_extractor(images=[Image.fromarray(img.astype('uint8')) for img in images], return_tensors=\"pt\").to(device)\n",
    "                with autocast():\n",
    "                    with torch.no_grad():\n",
    "                        batch_outputs = model(**batch_inputs)\n",
    "                return torch.nn.functional.softmax(batch_outputs.logits, dim=-1).cpu().numpy()\n",
    "\n",
    "            explainer = lime_image.LimeImageExplainer()\n",
    "            explanation = explainer.explain_instance(np.array(frame), \n",
    "                                                     batch_predict, \n",
    "                                                     top_labels=5, \n",
    "                                                     hide_color=0, \n",
    "                                                     num_samples=config[\"lime_num_samples\"])\n",
    "            \n",
    "            saliency_map = explanation.get_image_and_mask(top_pred, positive_only=True, num_features=10, hide_rest=False)[1]\n",
    "            saliency_map = (saliency_map - saliency_map.min()) / (saliency_map.max() - saliency_map.min())\n",
    "\n",
    "            # Calculate faithfulness\n",
    "            mask = saliency_map > 0.5\n",
    "            masked_frame = np.array(frame) * mask[..., np.newaxis]\n",
    "            masked_inputs = feature_extractor(images=Image.fromarray(masked_frame.astype('uint8')), return_tensors=\"pt\").to(device)\n",
    "            with autocast():\n",
    "                with torch.no_grad():\n",
    "                    masked_outputs = model(**masked_inputs)\n",
    "            masked_preds = torch.nn.functional.softmax(masked_outputs.logits, dim=-1)\n",
    "            masked_prediction = masked_preds[0, top_pred].item()\n",
    "            faithfulness = calculate_faithfulness(original_prediction, masked_prediction)\n",
    "\n",
    "            # Calculate monotonicity\n",
    "            percentages = np.linspace(0.1, 1.0, 10)\n",
    "            diffs = []\n",
    "            for p in percentages:\n",
    "                threshold = np.percentile(saliency_map, 100 * (1 - p))\n",
    "                temp_mask = saliency_map > threshold\n",
    "                temp_masked_frame = np.array(frame) * temp_mask[..., np.newaxis]\n",
    "                temp_inputs = feature_extractor(images=Image.fromarray(temp_masked_frame.astype('uint8')), return_tensors=\"pt\").to(device)\n",
    "                with autocast():\n",
    "                    with torch.no_grad():\n",
    "                        temp_outputs = model(**temp_inputs)\n",
    "                temp_preds = torch.nn.functional.softmax(temp_outputs.logits, dim=-1)\n",
    "                temp_prediction = temp_preds[0, top_pred].item()\n",
    "                diffs.append(abs(original_prediction - temp_prediction))\n",
    "            \n",
    "            logging.debug(f\"Percentages: {percentages}\")\n",
    "            logging.debug(f\"Diffs: {diffs}\")\n",
    "            \n",
    "            monotonicity = calculate_monotonicity(percentages, diffs)\n",
    "            if monotonicity is None:\n",
    "                logging.warning(f\"Unable to calculate monotonicity for frame {i+1}\")\n",
    "\n",
    "            results.append({\n",
    "                \"frame_index\": i,\n",
    "                \"top_prediction\": top_pred,\n",
    "                \"prediction_score\": original_prediction,\n",
    "                \"faithfulness\": faithfulness,\n",
    "                \"monotonicity\": monotonicity\n",
    "            })\n",
    "\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error processing frame {i+1} of {video_path}: {e}\")\n",
    "\n",
    "    return results\n",
    "\n",
    "def evaluate_lime(config):\n",
    "    video_labels = load_video_labels(config[\"video_list_path\"])\n",
    "    \n",
    "    faithfulness_scores = []\n",
    "    monotonicity_scores = []\n",
    "    computation_times = []\n",
    "\n",
    "    video_files = [f for f in os.listdir(config[\"video_directory\"]) if f.endswith('.mp4')][:config[\"num_eval_videos\"]]\n",
    "\n",
    "    for video_file in tqdm(video_files, desc=\"Evaluating videos\"):\n",
    "        video_path = os.path.join(config[\"video_directory\"], video_file)\n",
    "        video_name = os.path.splitext(video_file)[0]\n",
    "        true_label = video_labels.get(video_name)\n",
    "        \n",
    "        if true_label is None:\n",
    "            logging.warning(f\"Label not found for video: {video_file}\")\n",
    "            continue\n",
    "\n",
    "        logging.info(f\"Processing video: {video_path} (Label: {true_label})\")\n",
    "        \n",
    "        start_time = time.time()\n",
    "        results = process_and_explain(video_path, model, feature_extractor, config[\"num_frames_per_video\"])\n",
    "        end_time = time.time()\n",
    "        \n",
    "        if results:\n",
    "            frame_faithfulness = [r['faithfulness'] for r in results]\n",
    "            frame_monotonicity = [r['monotonicity'] for r in results if r['monotonicity'] is not None]\n",
    "            faithfulness_scores.extend(frame_faithfulness)\n",
    "            monotonicity_scores.extend(frame_monotonicity)\n",
    "            computation_times.append(end_time - start_time)\n",
    "            logging.info(f\"Processed {video_file}\")\n",
    "            logging.info(f\"  Faithfulness: {np.mean(frame_faithfulness):.4f} ± {np.std(frame_faithfulness):.4f}\")\n",
    "            if frame_monotonicity:\n",
    "                logging.info(f\"  Monotonicity: {np.mean(frame_monotonicity):.4f} ± {np.std(frame_monotonicity):.4f}\")\n",
    "            else:\n",
    "                logging.warning(f\"  No valid monotonicity scores for {video_file}\")\n",
    "        else:\n",
    "            logging.warning(f\"Failed to process {video_file}\")\n",
    "\n",
    "    # 计算结果时处理可能的空列表\n",
    "    results = {\n",
    "        \"faithfulness\": {\n",
    "            \"mean\": np.mean(faithfulness_scores) if faithfulness_scores else \"N/A\",\n",
    "            \"std\": np.std(faithfulness_scores) if faithfulness_scores else \"N/A\"\n",
    "        },\n",
    "        \"monotonicity\": {\n",
    "            \"mean\": np.mean(monotonicity_scores) if monotonicity_scores else \"N/A\",\n",
    "            \"std\": np.std(monotonicity_scores) if monotonicity_scores else \"N/A\"\n",
    "        },\n",
    "        \"computation_time\": {\n",
    "            \"mean\": np.mean(computation_times) if computation_times else \"N/A\",\n",
    "            \"std\": np.std(computation_times) if computation_times else \"N/A\"\n",
    "        }\n",
    "    }\n",
    "\n",
    "    with open(os.path.join(config[\"results_folder\"], \"lime_evaluation_results.json\"), \"w\") as f:\n",
    "        json.dump(results, f, indent=4)\n",
    "\n",
    "    print(f\"Faithfulness: {results['faithfulness']['mean']:.4f} ± {results['faithfulness']['std']:.4f}\")\n",
    "    print(f\"Monotonicity: {results['monotonicity']['mean']:.4f} ± {results['monotonicity']['std']:.4f}\")\n",
    "    print(f\"Computation Time: {results['computation_time']['mean']:.2f} ± {results['computation_time']['std']:.2f} seconds\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    evaluate_lime(config)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mmxai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
