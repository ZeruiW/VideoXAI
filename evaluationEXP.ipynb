{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "STAA one label test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/z/miniconda3/envs/mmxai/lib/python3.12/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "2024-10-02 14:39:53,961 - INFO - Loaded 19796 video labels\n",
      "2024-10-02 14:39:53,962 - INFO - Unique labels in the dataset: {0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399}\n",
      "2024-10-02 14:39:53,963 - INFO - Found 50 videos for label 188\n",
      "Processing videos:   0%|          | 0/50 [00:00<?, ?it/s]/home/z/miniconda3/envs/mmxai/lib/python3.12/site-packages/transformers/feature_extraction_utils.py:141: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:274.)\n",
      "  return torch.tensor(value)\n",
      "Processing frames:   7%|▋         | 21/300 [00:02<00:29,  9.59it/s]\n",
      "Processing videos:   0%|          | 0/50 [00:02<?, ?it/s]\n",
      "2024-10-02 14:39:56,159 - ERROR - An error occurred: Unable to create tensor, you should probably activate padding with 'padding=True' to have batched tensors with the same length.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/z/miniconda3/envs/mmxai/lib/python3.12/site-packages/transformers/feature_extraction_utils.py\", line 182, in convert_to_tensors\n",
      "    tensor = as_tensor(value)\n",
      "             ^^^^^^^^^^^^^^^^\n",
      "  File \"/home/z/miniconda3/envs/mmxai/lib/python3.12/site-packages/transformers/feature_extraction_utils.py\", line 141, in as_tensor\n",
      "    return torch.tensor(value)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_30162/759591552.py\", line 307, in <module>\n",
      "    process_videos(config)\n",
      "  File \"/tmp/ipykernel_30162/759591552.py\", line 290, in process_videos\n",
      "    predicted_label, frames_dir, json_path, heatmap_video_path = process_video(video_path, video_output_dir, extractor)\n",
      "                                                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_30162/759591552.py\", line 83, in process_video\n",
      "    spatial_attention = multi_scale_attention(extractor, frames)\n",
      "                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_30162/759591552.py\", line 44, in multi_scale_attention\n",
      "    attention, _ = extractor.extract_attention(scaled_frames)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_30162/759591552.py\", line 24, in extract_attention\n",
      "    inputs = self.image_processor(frames, return_tensors=\"pt\").to(self.device)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/z/miniconda3/envs/mmxai/lib/python3.12/site-packages/transformers/image_processing_utils.py\", line 551, in __call__\n",
      "    return self.preprocess(images, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/z/miniconda3/envs/mmxai/lib/python3.12/site-packages/transformers/models/videomae/image_processing_videomae.py\", line 364, in preprocess\n",
      "    return BatchFeature(data=data, tensor_type=return_tensors)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/z/miniconda3/envs/mmxai/lib/python3.12/site-packages/transformers/feature_extraction_utils.py\", line 78, in __init__\n",
      "    self.convert_to_tensors(tensor_type=tensor_type)\n",
      "  File \"/home/z/miniconda3/envs/mmxai/lib/python3.12/site-packages/transformers/feature_extraction_utils.py\", line 188, in convert_to_tensors\n",
      "    raise ValueError(\n",
      "ValueError: Unable to create tensor, you should probably activate padding with 'padding=True' to have batched tensors with the same length.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import av\n",
    "import torch\n",
    "import numpy as np\n",
    "from transformers import TimesformerForVideoClassification, AutoImageProcessor\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import cv2\n",
    "import json\n",
    "import logging\n",
    "from scipy.signal import savgol_filter, find_peaks\n",
    "import matplotlib.gridspec as gridspec\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "class AttentionExtractor:\n",
    "    def __init__(self, model_name, device='cuda' if torch.cuda.is_available() else 'cpu'):\n",
    "        self.model = TimesformerForVideoClassification.from_pretrained(model_name)\n",
    "        self.model.to(device)\n",
    "        self.device = device\n",
    "        self.image_processor = AutoImageProcessor.from_pretrained(model_name)\n",
    "\n",
    "    def extract_attention(self, frames):\n",
    "        inputs = self.image_processor(frames, return_tensors=\"pt\").to(self.device)\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(**inputs, output_attentions=True)\n",
    "        last_layer_attention = outputs.attentions[-1]\n",
    "        spatial_attention = last_layer_attention.mean(1)\n",
    "        return spatial_attention.cpu().numpy(), outputs.logits.cpu().numpy()\n",
    "\n",
    "    def apply_attention_heatmap(self, frame, attention):\n",
    "        att_map = attention[1:].reshape(int(np.sqrt(attention.shape[0]-1)), -1)\n",
    "        att_resized = cv2.resize(att_map, (frame.shape[1], frame.shape[0]))\n",
    "        att_norm = (att_resized - att_resized.min()) / (att_resized.max() - att_resized.min())\n",
    "        heatmap = cv2.applyColorMap(np.uint8(255 * att_norm), cv2.COLORMAP_JET)\n",
    "        blend = cv2.addWeighted(frame, 0.7, heatmap, 0.3, 0)\n",
    "        return blend\n",
    "\n",
    "def multi_scale_attention(extractor, frames):\n",
    "    scales = [0.5, 0.75, 1.0, 1.25, 1.5]\n",
    "    attentions = []\n",
    "    for scale in scales:\n",
    "        scaled_frames = [cv2.resize(frame, None, fx=scale, fy=scale, interpolation=cv2.INTER_LINEAR) for frame in frames]\n",
    "        attention, _ = extractor.extract_attention(scaled_frames)\n",
    "        attentions.append(attention)\n",
    "    return np.mean(attentions, axis=0)\n",
    "\n",
    "def exponential_smoothing(data, alpha=0.3):\n",
    "    smoothed = [data[0]]\n",
    "    for i in range(1, len(data)):\n",
    "        smoothed.append(alpha * data[i] + (1 - alpha) * smoothed[-1])\n",
    "    return smoothed\n",
    "\n",
    "def process_video(video_path, output_dir, extractor, sampling_rate=2, temporal_smoothing_window=5):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    frames_dir = os.path.join(output_dir, 'frames')\n",
    "    os.makedirs(frames_dir, exist_ok=True)\n",
    "\n",
    "    container = av.open(video_path)\n",
    "    video_stream = container.streams.video[0]\n",
    "    fps = video_stream.average_rate\n",
    "    total_frames = video_stream.frames\n",
    "    \n",
    "    # 创建输出视频文件\n",
    "    output_path = os.path.join(output_dir, f\"{os.path.basename(video_path).split('.')[0]}_heatmap.mp4\")\n",
    "    output = av.open(output_path, mode='w')\n",
    "    output_stream = output.add_stream('h264', rate=fps)\n",
    "    output_stream.width = video_stream.width\n",
    "    output_stream.height = video_stream.height\n",
    "    output_stream.pix_fmt = 'yuv420p'\n",
    "\n",
    "    frames = []\n",
    "    attention_data = []\n",
    "    frame_count = 0\n",
    "    attention_buffer = []\n",
    "    all_logits = []\n",
    "\n",
    "    for frame in tqdm(container.decode(video=0), desc=\"Processing frames\", total=total_frames):\n",
    "        frame_rgb = frame.to_rgb().to_ndarray()\n",
    "        frames.append(frame_rgb)\n",
    "        \n",
    "        if len(frames) == 8:\n",
    "            spatial_attention = multi_scale_attention(extractor, frames)\n",
    "            logits = extractor.extract_attention(frames)[1]\n",
    "            all_logits.append(logits)\n",
    "            \n",
    "            for i in range(8):\n",
    "                attention = spatial_attention[0, i+1]\n",
    "                attention_buffer.append(attention)\n",
    "                \n",
    "                if len(attention_buffer) >= temporal_smoothing_window:\n",
    "                    smoothed_attention = np.mean(attention_buffer[-temporal_smoothing_window:], axis=0)\n",
    "                    heatmap_frame = extractor.apply_attention_heatmap(frames[i], smoothed_attention)\n",
    "                    \n",
    "                    if frame_count % sampling_rate == 0:\n",
    "                        frame_filename = f\"{os.path.basename(video_path).split('.')[0]}_frame_{frame_count+1}_spatial_attention.png\"\n",
    "                        cv2.imwrite(os.path.join(frames_dir, frame_filename), cv2.cvtColor(heatmap_frame, cv2.COLOR_RGB2BGR))\n",
    "                        \n",
    "                        attention_data.append({\n",
    "                            \"frame_index\": frame_count,\n",
    "                            \"max_attention\": float(smoothed_attention[1:].max()),\n",
    "                            \"min_attention\": float(smoothed_attention[1:].min()),\n",
    "                            \"mean_attention\": float(smoothed_attention[1:].mean())\n",
    "                        })\n",
    "                    \n",
    "                    # 将每一帧都写入输出视频\n",
    "                    out_frame = av.VideoFrame.from_ndarray(heatmap_frame, format='rgb24')\n",
    "                    packet = output_stream.encode(out_frame)\n",
    "                    output.mux(packet)\n",
    "                \n",
    "                frame_count += 1\n",
    "            \n",
    "            frames = frames[7:]\n",
    "\n",
    "    # Process remaining frames\n",
    "    if frames:\n",
    "        padding = [frames[-1]] * (8 - len(frames))\n",
    "        spatial_attention = multi_scale_attention(extractor, frames + padding)\n",
    "        logits = extractor.extract_attention(frames + padding)[1]\n",
    "        all_logits.append(logits)\n",
    "        \n",
    "        for i in range(len(frames)):\n",
    "            attention = spatial_attention[0, i+1]\n",
    "            attention_buffer.append(attention)\n",
    "            \n",
    "            smoothed_attention = np.mean(attention_buffer[-temporal_smoothing_window:], axis=0)\n",
    "            heatmap_frame = extractor.apply_attention_heatmap(frames[i], smoothed_attention)\n",
    "            \n",
    "            if frame_count % sampling_rate == 0:\n",
    "                frame_filename = f\"{os.path.basename(video_path).split('.')[0]}_frame_{frame_count+1}_spatial_attention.png\"\n",
    "                cv2.imwrite(os.path.join(frames_dir, frame_filename), cv2.cvtColor(heatmap_frame, cv2.COLOR_RGB2BGR))\n",
    "                \n",
    "                attention_data.append({\n",
    "                    \"frame_index\": frame_count,\n",
    "                    \"max_attention\": float(smoothed_attention[1:].max()),\n",
    "                    \"min_attention\": float(smoothed_attention[1:].min()),\n",
    "                    \"mean_attention\": float(smoothed_attention[1:].mean())\n",
    "                })\n",
    "            \n",
    "            # 将每一帧都写入输出视频\n",
    "            out_frame = av.VideoFrame.from_ndarray(heatmap_frame, format='rgb24')\n",
    "            packet = output_stream.encode(out_frame)\n",
    "            output.mux(packet)\n",
    "            \n",
    "            frame_count += 1\n",
    "\n",
    "    # Flush encoder\n",
    "    packet = output_stream.encode(None)\n",
    "    output.mux(packet)\n",
    "    output.close()\n",
    "\n",
    "    # Apply exponential smoothing to attention data\n",
    "    smoothed_attention = exponential_smoothing([frame['mean_attention'] for frame in attention_data])\n",
    "    for i, att in enumerate(smoothed_attention):\n",
    "        attention_data[i]['mean_attention'] = att\n",
    "\n",
    "    # Save attention data\n",
    "    with open(os.path.join(output_dir, f\"{os.path.basename(video_path).split('.')[0]}_rs.json\"), 'w') as f:\n",
    "        json.dump(attention_data, f)\n",
    "\n",
    "    overall_logits = np.mean(all_logits, axis=0)\n",
    "    predicted_label = int(np.argmax(overall_logits))\n",
    "\n",
    "    return predicted_label, frames_dir, os.path.join(output_dir, f\"{os.path.basename(video_path).split('.')[0]}_rs.json\"), output_path\n",
    "\n",
    "def create_sample_frames_visualization(video_name, num_segments=8, results_dir='attention_results'):\n",
    "    try:\n",
    "        # Load data\n",
    "        json_path = os.path.join(results_dir, f\"{video_name}_rs.json\")\n",
    "        if not os.path.exists(json_path):\n",
    "            raise FileNotFoundError(f\"JSON file not found: {json_path}\")\n",
    "        \n",
    "        with open(json_path, 'r') as f:\n",
    "            attention_data = json.load(f)\n",
    "        \n",
    "        # Extract temporal attention\n",
    "        temporal_attention = np.array([frame['mean_attention'] for frame in attention_data])\n",
    "        frame_indices = np.array([frame['frame_index'] for frame in attention_data])\n",
    "        \n",
    "        # Apply Savitzky-Golay filter for additional smoothing\n",
    "        window_length = min(len(temporal_attention) // 2 * 2 + 1, 21)  # Must be odd and not exceed data length\n",
    "        temporal_attention_smoothed = savgol_filter(temporal_attention, window_length, 3)\n",
    "        \n",
    "        # Normalize temporal attention\n",
    "        temporal_attention_smoothed = (temporal_attention_smoothed - temporal_attention_smoothed.min()) / (temporal_attention_smoothed.max() - temporal_attention_smoothed.min())\n",
    "        \n",
    "        # Select key frames based on local maxima\n",
    "        peaks, _ = find_peaks(temporal_attention_smoothed, distance=len(temporal_attention_smoothed)//num_segments)\n",
    "        if len(peaks) < num_segments:\n",
    "            additional_frames = np.linspace(0, len(temporal_attention_smoothed)-1, num_segments-len(peaks), dtype=int)\n",
    "            key_frame_indices = np.sort(np.concatenate([peaks, additional_frames]))\n",
    "        else:\n",
    "            key_frame_indices = peaks[:num_segments]\n",
    "        \n",
    "        # Create figure\n",
    "        fig = plt.figure(figsize=(16, 9))  # 16:9 aspect ratio\n",
    "        gs = gridspec.GridSpec(2, 1, height_ratios=[1, 2])\n",
    "        \n",
    "        # Plot temporal saliency\n",
    "        ax1 = plt.subplot(gs[0])\n",
    "        ax1.plot(frame_indices, temporal_attention_smoothed, color='blue', alpha=0.7, linewidth=2)\n",
    "        ax1.scatter(frame_indices[key_frame_indices], temporal_attention_smoothed[key_frame_indices], color='red', s=100, zorder=5)\n",
    "        for idx in key_frame_indices:\n",
    "            ax1.axvline(x=frame_indices[idx], color='gray', linestyle='--', alpha=0.5)\n",
    "        ax1.set_xlabel(\"Frame Number\", fontsize=18)\n",
    "        ax1.set_ylabel(\"Temporal Saliency\", fontsize=18)\n",
    "        ax1.set_xlim(frame_indices[0], frame_indices[-1])\n",
    "        ax1.set_ylim(0, 1)\n",
    "        ax1.tick_params(axis='both', which='major', labelsize=14)\n",
    "        ax1.set_title(f\"Temporal Saliency and Key Frames - {video_name}\", fontsize=20)\n",
    "        \n",
    "        # Display key frames\n",
    "        ax2 = plt.subplot(gs[1])\n",
    "        ax2.axis('off')\n",
    "        frames_loaded = 0\n",
    "        for i, idx in enumerate(key_frame_indices):\n",
    "            frame_number = frame_indices[idx]\n",
    "            frame_path = os.path.join(results_dir, 'frames', f\"{video_name}_frame_{frame_number}_spatial_attention.png\")\n",
    "            \n",
    "            if os.path.exists(frame_path):\n",
    "                frame = cv2.imread(frame_path)\n",
    "                if frame is not None:\n",
    "                    frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "                    ax_sub = ax2.inset_axes([i/num_segments, 0, 1/num_segments - 0.01, 1], transform=ax2.transAxes)\n",
    "                    ax_sub.imshow(frame)\n",
    "                    ax_sub.axis('off')\n",
    "                    ax_sub.set_title(f\"Frame {frame_number}\", fontsize=14)\n",
    "                    frames_loaded += 1\n",
    "                else:\n",
    "                    print(f\"Failed to load frame: {frame_path}\")\n",
    "            else:\n",
    "                print(f\"Frame not found: {frame_path}\")\n",
    "        \n",
    "        if frames_loaded == 0:\n",
    "            print(f\"No frames were loaded for {video_name}. Check the 'frames' directory and file names.\")\n",
    "        else:\n",
    "            print(f\"Successfully loaded {frames_loaded} frames for {video_name}.\")\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        output_path = os.path.join(results_dir, f\"{video_name}_sample_frames.png\")\n",
    "        plt.savefig(output_path, dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        \n",
    "        print(f\"Sample frames visualization saved to: {output_path}\")\n",
    "        return output_path\n",
    "    except Exception as e:\n",
    "        print(f\"Error in create_sample_frames_visualization for video {video_name}: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        raise\n",
    "\n",
    "def load_video_labels(label_file):\n",
    "    video_labels = {}\n",
    "    with open(label_file, 'r') as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split()\n",
    "            if len(parts) == 2:\n",
    "                video_name, label = parts\n",
    "                video_labels[video_name.split('.')[0]] = int(label)  # Remove .mp4 extension\n",
    "            else:\n",
    "                logging.warning(f\"Skipping invalid line: {line.strip()}\")\n",
    "    logging.info(f\"Loaded {len(video_labels)} video labels\")\n",
    "    logging.info(f\"Unique labels in the dataset: {set(video_labels.values())}\")\n",
    "    return video_labels\n",
    "\n",
    "def get_videos_by_label(video_labels, target_label):\n",
    "    matching_videos = [video for video, label in video_labels.items() if label == target_label]\n",
    "    logging.info(f\"Found {len(matching_videos)} videos for label {target_label}\")\n",
    "    return matching_videos\n",
    "\n",
    "def process_videos(config):\n",
    "    extractor = AttentionExtractor(config['model_name'])\n",
    "    \n",
    "    video_labels = load_video_labels(config['label_file'])\n",
    "    \n",
    "    target_videos = get_videos_by_label(video_labels, config['target_label'])\n",
    "    \n",
    "    if not target_videos:\n",
    "        logging.warning(f\"No videos found for label {config['target_label']}\")\n",
    "        return\n",
    "\n",
    "    for video_name in tqdm(target_videos, desc=\"Processing videos\"):\n",
    "        video_path = os.path.join(config['video_directory'], video_name + '.mp4')\n",
    "        \n",
    "        if not os.path.exists(video_path):\n",
    "            logging.warning(f\"Video file not found: {video_path}\")\n",
    "            continue\n",
    "        \n",
    "        video_output_dir = os.path.join(config['output_directory'], video_name)\n",
    "        predicted_label, frames_dir, json_path, heatmap_video_path = process_video(video_path, video_output_dir, extractor)\n",
    "        \n",
    "        create_sample_frames_visualization(video_name, results_dir=video_output_dir)\n",
    "        \n",
    "        print(f\"Processed {video_name}\")\n",
    "        print(f\"Predicted Label: {predicted_label}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    config = {\n",
    "        'model_name': 'facebook/timesformer-base-finetuned-k400',\n",
    "        'video_directory': 'archive/videos_val',\n",
    "        'output_directory': 'attention_results/188',\n",
    "        'label_file': 'archive/kinetics400_val_list_videos.txt',\n",
    "        'target_label': int(input(\"Enter the target label number: \"))\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        process_videos(config)\n",
    "    except Exception as e:\n",
    "        logging.error(f\"An error occurred: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing videos: 100%|██████████| 4/4 [00:08<00:00,  2.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall Faithfulness: 0.76 ± 0.03\n",
      "Overall Monotonicity (Kendall's Tau): 0.58 ± 0.22\n",
      "Average Computation Time: 0.16 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import av\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import os\n",
    "from transformers import TimesformerForVideoClassification, AutoImageProcessor\n",
    "from scipy.stats import kendalltau\n",
    "import json\n",
    "\n",
    "class AttentionExtractor:\n",
    "    def __init__(self, model_name, device='cuda' if torch.cuda.is_available() else 'cpu'):\n",
    "        self.model = TimesformerForVideoClassification.from_pretrained(model_name)\n",
    "        self.model.to(device)\n",
    "        self.device = device\n",
    "        self.image_processor = AutoImageProcessor.from_pretrained(model_name)\n",
    "\n",
    "    def extract_attention(self, frames):\n",
    "        # 确保帧数据的形状正确\n",
    "        if frames.ndim == 3:\n",
    "            frames = frames[np.newaxis, ...]\n",
    "        inputs = self.image_processor(list(frames), return_tensors=\"pt\").to(self.device)\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(**inputs, output_attentions=True)\n",
    "        last_layer_attention = outputs.attentions[-1]\n",
    "        spatial_attention = last_layer_attention.mean(1)\n",
    "        return outputs.logits.cpu().numpy(), spatial_attention.cpu().numpy()\n",
    "\n",
    "def load_video(video_path, num_frames=8):\n",
    "    container = av.open(video_path)\n",
    "    video_stream = container.streams.video[0]\n",
    "    \n",
    "    frames = []\n",
    "    for frame in container.decode(video=0):\n",
    "        frames.append(frame.to_rgb().to_ndarray())\n",
    "        if len(frames) == num_frames:\n",
    "            break\n",
    "    \n",
    "    # 如果视频帧数少于 num_frames，则重复最后一帧\n",
    "    while len(frames) < num_frames:\n",
    "        frames.append(frames[-1])\n",
    "    \n",
    "    # 确保帧数组的形状正确\n",
    "    frames = np.stack(frames)\n",
    "    \n",
    "    return frames\n",
    "\n",
    "def calculate_faithfulness(extractor, video_path):\n",
    "    frames = load_video(video_path)\n",
    "    original_prediction, attention = extractor.extract_attention(frames)\n",
    "    \n",
    "    flat_attention = attention.flatten()\n",
    "    k = int(0.5 * len(flat_attention))\n",
    "    top_k_indices = np.argsort(flat_attention)[-k:]\n",
    "    \n",
    "    masked_frames = frames.copy()\n",
    "    masked_frames.flat[top_k_indices] = 0\n",
    "    \n",
    "    masked_prediction, _ = extractor.extract_attention(masked_frames)\n",
    "    \n",
    "    faithfulness = 1 - np.abs(original_prediction - masked_prediction).mean()\n",
    "    return faithfulness\n",
    "\n",
    "def calculate_monotonicity(extractor, video_path):\n",
    "    frames = load_video(video_path)\n",
    "    original_prediction, attention = extractor.extract_attention(frames)\n",
    "    \n",
    "    flat_attention = attention.flatten()\n",
    "    percentages = np.arange(0.1, 1.0, 0.1)\n",
    "    diffs = []\n",
    "    \n",
    "    for p in percentages:\n",
    "        k = int(p * len(flat_attention))\n",
    "        top_k_indices = np.argsort(flat_attention)[-k:]\n",
    "        \n",
    "        masked_frames = frames.copy()\n",
    "        masked_frames.flat[top_k_indices] = 0\n",
    "        \n",
    "        masked_prediction, _ = extractor.extract_attention(masked_frames)\n",
    "        diff = np.abs(original_prediction - masked_prediction).mean()\n",
    "        diffs.append(diff)\n",
    "    \n",
    "    tau, _ = kendalltau(percentages, diffs)\n",
    "    return tau\n",
    "\n",
    "def load_video_labels(label_file):\n",
    "    video_labels = {}\n",
    "    with open(label_file, 'r') as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split()\n",
    "            if len(parts) == 2:\n",
    "                video_name, label = parts\n",
    "                video_labels[video_name.split('.')[0]] = int(label)\n",
    "    return video_labels\n",
    "\n",
    "def evaluate_staa(config):\n",
    "    extractor = AttentionExtractor(config['model_name'])\n",
    "    video_labels = load_video_labels(config['label_file'])\n",
    "    \n",
    "    results = {}\n",
    "    video_files = [f for f in os.listdir(config['video_directory']) if f.endswith('.mp4')]\n",
    "    \n",
    "    for video_file in tqdm(video_files, desc=\"Processing videos\"):\n",
    "        video_name = video_file.split('.')[0]\n",
    "        video_path = os.path.join(config['video_directory'], video_file)\n",
    "        \n",
    "        if video_name not in video_labels:\n",
    "            print(f\"Warning: No label found for {video_name}. Skipping.\")\n",
    "            continue\n",
    "        \n",
    "        true_label = video_labels[video_name]\n",
    "        \n",
    "        # Calculate faithfulness\n",
    "        faithfulness = calculate_faithfulness(extractor, video_path)\n",
    "        \n",
    "        # Calculate monotonicity using Kendall's Tau\n",
    "        monotonicity = calculate_monotonicity(extractor, video_path)\n",
    "        \n",
    "        # Calculate computation time\n",
    "        start_time = time.time()\n",
    "        _ = extractor.extract_attention(load_video(video_path))\n",
    "        computation_time = time.time() - start_time\n",
    "        \n",
    "        results[video_name] = {\n",
    "            \"true_label\": true_label,\n",
    "            \"faithfulness\": faithfulness,\n",
    "            \"monotonicity\": monotonicity,\n",
    "            \"computation_time\": computation_time\n",
    "        }\n",
    "    \n",
    "    # Calculate overall statistics\n",
    "    faithfulness_scores = [r['faithfulness'] for r in results.values()]\n",
    "    monotonicity_scores = [r['monotonicity'] for r in results.values()]\n",
    "    computation_times = [r['computation_time'] for r in results.values()]\n",
    "    \n",
    "    print(f\"Overall Faithfulness: {np.mean(faithfulness_scores):.2f} ± {np.std(faithfulness_scores):.2f}\")\n",
    "    print(f\"Overall Monotonicity (Kendall's Tau): {np.mean(monotonicity_scores):.2f} ± {np.std(monotonicity_scores):.2f}\")\n",
    "    print(f\"Average Computation Time: {np.mean(computation_times):.2f} seconds\")\n",
    "    \n",
    "    # Save results to a JSON file\n",
    "    with open(os.path.join(config['output_directory'], 'evaluation_results.json'), 'w') as f:\n",
    "        json.dump(results, f, indent=4)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    config = {\n",
    "        'model_name': 'facebook/timesformer-base-finetuned-k400',\n",
    "        'video_directory': 'archive/example',\n",
    "        'output_directory': 'archive/examplers',\n",
    "        'label_file': 'archive/kinetics400_val_list_videos.txt',\n",
    "    }\n",
    "    \n",
    "    evaluate_staa(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "修改STAA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing videos:   0%|          | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frames shape: (8, 320, 570, 3)\n",
      "Attention shape: (8, 197, 197)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing videos:  25%|██▌       | 1/4 [00:02<00:07,  2.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frames shape: (8, 320, 426, 3)\n",
      "Attention shape: (8, 197, 197)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing videos:  50%|█████     | 2/4 [00:05<00:05,  2.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frames shape: (8, 320, 426, 3)\n",
      "Attention shape: (8, 197, 197)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing videos:  75%|███████▌  | 3/4 [00:07<00:02,  2.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frames shape: (8, 320, 426, 3)\n",
      "Attention shape: (8, 197, 197)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing videos: 100%|██████████| 4/4 [00:09<00:00,  2.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall Faithfulness: -1.15 ± 0.15\n",
      "Overall Monotonicity (Kendall's Tau): -0.89 ± 0.04\n",
      "Average Computation Time: 0.17 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import av\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import os\n",
    "from transformers import TimesformerForVideoClassification, AutoImageProcessor\n",
    "from scipy.stats import kendalltau\n",
    "import json\n",
    "\n",
    "class AttentionExtractor:\n",
    "    def __init__(self, model_name, device='cuda' if torch.cuda.is_available() else 'cpu'):\n",
    "        self.model = TimesformerForVideoClassification.from_pretrained(model_name)\n",
    "        self.model.to(device)\n",
    "        self.device = device\n",
    "        self.image_processor = AutoImageProcessor.from_pretrained(model_name)\n",
    "\n",
    "    def extract_attention(self, frames):\n",
    "        # 确保帧数据的形状正确\n",
    "        if frames.ndim == 3:\n",
    "            frames = frames[np.newaxis, ...]\n",
    "        inputs = self.image_processor(list(frames), return_tensors=\"pt\").to(self.device)\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(**inputs, output_attentions=True)\n",
    "        last_layer_attention = outputs.attentions[-1]\n",
    "        spatial_attention = last_layer_attention.mean(1)\n",
    "        return outputs.logits.cpu().numpy(), spatial_attention.cpu().numpy()\n",
    "\n",
    "def load_video(video_path, num_frames=8):\n",
    "    container = av.open(video_path)\n",
    "    video_stream = container.streams.video[0]\n",
    "    \n",
    "    frames = []\n",
    "    for frame in container.decode(video=0):\n",
    "        frames.append(frame.to_rgb().to_ndarray())\n",
    "        if len(frames) == num_frames:\n",
    "            break\n",
    "    \n",
    "    # 如果视频帧数少于 num_frames，则重复最后一帧\n",
    "    while len(frames) < num_frames:\n",
    "        frames.append(frames[-1])\n",
    "    \n",
    "    # 确保帧数组的形状正确\n",
    "    frames = np.stack(frames)\n",
    "    \n",
    "    return frames\n",
    "\n",
    "\n",
    "def calculate_faithfulness(extractor, video_path):\n",
    "    frames = load_video(video_path)\n",
    "    original_prediction, attention = extractor.extract_attention(frames)\n",
    "    \n",
    "    print(\"Frames shape:\", frames.shape)\n",
    "    print(\"Attention shape:\", attention.shape)\n",
    "    \n",
    "    # 获取帧的形状\n",
    "    frame_shape = frames.shape\n",
    "    attention_shape = attention.shape\n",
    "    \n",
    "    # 计算要保留的像素数量\n",
    "    k = int(0.5 * attention.size)\n",
    "    \n",
    "    # 获取最重要的50%的索引\n",
    "    flat_attention = attention.flatten()\n",
    "    top_k_indices = np.argsort(flat_attention)[-k:]\n",
    "    \n",
    "    # 创建掩码\n",
    "    mask = np.zeros_like(flat_attention)\n",
    "    mask[top_k_indices] = 1\n",
    "    mask = mask.reshape(attention_shape)\n",
    "    \n",
    "    # 将mask调整为与帧相同的大小\n",
    "    mask_resized = np.zeros(frame_shape[:-1])  # 不包括颜色通道\n",
    "    for i in range(frame_shape[0]):  # 对每一帧\n",
    "        mask_resized[i] = cv2.resize(mask[i], (frame_shape[2], frame_shape[1]))\n",
    "    \n",
    "    # 应用掩码到原始帧\n",
    "    masked_frames = frames.copy()\n",
    "    for i in range(frame_shape[-1]):  # 对每个颜色通道应用掩码\n",
    "        masked_frames[..., i] = frames[..., i] * mask_resized\n",
    "    \n",
    "    masked_prediction, _ = extractor.extract_attention(masked_frames)\n",
    "    \n",
    "    # 计算faithfulness\n",
    "    faithfulness = 1 - np.abs(original_prediction - masked_prediction).mean()\n",
    "    return faithfulness\n",
    "\n",
    "def calculate_monotonicity(extractor, video_path):\n",
    "    frames = load_video(video_path)\n",
    "    original_prediction, attention = extractor.extract_attention(frames)\n",
    "    \n",
    "    frame_shape = frames.shape\n",
    "    attention_shape = attention.shape\n",
    "    \n",
    "    flat_attention = attention.flatten()\n",
    "    percentages = np.arange(0.1, 1.0, 0.1)\n",
    "    diffs = []\n",
    "    \n",
    "    for p in percentages:\n",
    "        k = int(p * flat_attention.size)\n",
    "        top_k_indices = np.argsort(flat_attention)[-k:]\n",
    "        \n",
    "        # 创建掩码\n",
    "        mask = np.zeros_like(flat_attention)\n",
    "        mask[top_k_indices] = 1\n",
    "        mask = mask.reshape(attention_shape)\n",
    "        \n",
    "        # 将mask调整为与帧相同的大小\n",
    "        mask_resized = np.zeros(frame_shape[:-1])  # 不包括颜色通道\n",
    "        for i in range(frame_shape[0]):  # 对每一帧\n",
    "            mask_resized[i] = cv2.resize(mask[i], (frame_shape[2], frame_shape[1]))\n",
    "        \n",
    "        # 应用掩码到原始帧\n",
    "        masked_frames = frames.copy()\n",
    "        for i in range(frame_shape[-1]):  # 对每个颜色通道应用掩码\n",
    "            masked_frames[..., i] = frames[..., i] * mask_resized\n",
    "        \n",
    "        masked_prediction, _ = extractor.extract_attention(masked_frames)\n",
    "        diff = np.abs(original_prediction - masked_prediction).mean()\n",
    "        diffs.append(diff)\n",
    "    \n",
    "    tau, _ = kendalltau(percentages, diffs)\n",
    "    return tau\n",
    "\n",
    "def load_video_labels(label_file):\n",
    "    video_labels = {}\n",
    "    with open(label_file, 'r') as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split()\n",
    "            if len(parts) == 2:\n",
    "                video_name, label = parts\n",
    "                video_labels[video_name.split('.')[0]] = int(label)\n",
    "    return video_labels\n",
    "\n",
    "def evaluate_staa(config):\n",
    "    extractor = AttentionExtractor(config['model_name'])\n",
    "    video_labels = load_video_labels(config['label_file'])\n",
    "    \n",
    "    results = {}\n",
    "    video_files = [f for f in os.listdir(config['video_directory']) if f.endswith('.mp4')]\n",
    "    \n",
    "    for video_file in tqdm(video_files, desc=\"Processing videos\"):\n",
    "        video_name = video_file.split('.')[0]\n",
    "        video_path = os.path.join(config['video_directory'], video_file)\n",
    "        \n",
    "        if video_name not in video_labels:\n",
    "            print(f\"Warning: No label found for {video_name}. Skipping.\")\n",
    "            continue\n",
    "        \n",
    "        true_label = video_labels[video_name]\n",
    "        \n",
    "        # Calculate faithfulness\n",
    "        faithfulness = calculate_faithfulness(extractor, video_path)\n",
    "        \n",
    "        # Calculate monotonicity using Kendall's Tau\n",
    "        monotonicity = calculate_monotonicity(extractor, video_path)\n",
    "        \n",
    "        # Calculate computation time\n",
    "        start_time = time.time()\n",
    "        _ = extractor.extract_attention(load_video(video_path))\n",
    "        computation_time = time.time() - start_time\n",
    "        \n",
    "        results[video_name] = {\n",
    "            \"true_label\": true_label,\n",
    "            \"faithfulness\": faithfulness,\n",
    "            \"monotonicity\": monotonicity,\n",
    "            \"computation_time\": computation_time\n",
    "        }\n",
    "    \n",
    "    # Calculate overall statistics\n",
    "    faithfulness_scores = [r['faithfulness'] for r in results.values()]\n",
    "    monotonicity_scores = [r['monotonicity'] for r in results.values()]\n",
    "    computation_times = [r['computation_time'] for r in results.values()]\n",
    "    \n",
    "    print(f\"Overall Faithfulness: {np.mean(faithfulness_scores):.2f} ± {np.std(faithfulness_scores):.2f}\")\n",
    "    print(f\"Overall Monotonicity (Kendall's Tau): {np.mean(monotonicity_scores):.2f} ± {np.std(monotonicity_scores):.2f}\")\n",
    "    print(f\"Average Computation Time: {np.mean(computation_times):.2f} seconds\")\n",
    "    \n",
    "    # Save results to a JSON file\n",
    "    with open(os.path.join(config['output_directory'], 'evaluation_results.json'), 'w') as f:\n",
    "        json.dump(results, f, indent=4)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    config = {\n",
    "        'model_name': 'facebook/timesformer-base-finetuned-k400',\n",
    "        'video_directory': 'archive/example',\n",
    "        'output_directory': 'archive/examplers',\n",
    "        'label_file': 'archive/kinetics400_val_list_videos.txt',\n",
    "    }\n",
    "    \n",
    "    evaluate_staa(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SHAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration. Please open a PR/issue to update `preprocessor_config.json` to use `image_processor_type` instead of `feature_extractor_type`. This warning will be removed in v4.40.\n",
      "Evaluating videos: 100%|██████████| 10/10 [00:59<00:00,  5.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faithfulness: 0.1485 ± 0.1543\n",
      "Monotonicity: 0.5803 ± 0.0866\n",
      "Computation Time: 5.92 ± 0.12 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import av\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoImageProcessor, TimesformerForVideoClassification\n",
    "from tqdm import tqdm\n",
    "from scipy.stats import kendalltau\n",
    "import time\n",
    "import json\n",
    "import random\n",
    "from collections import defaultdict\n",
    "\n",
    "class VideoProcessor:\n",
    "    def __init__(self, model_name, image_processor_name, device='cuda'):\n",
    "        self.model = TimesformerForVideoClassification.from_pretrained(model_name)\n",
    "        self.image_processor = AutoImageProcessor.from_pretrained(image_processor_name)\n",
    "        self.device = device\n",
    "        self.model.to(device)\n",
    "\n",
    "    def split_video_into_segments(self, container, n_segments=8, frames_per_segment=16):\n",
    "        frame_list = [frame.to_image() for frame in container.decode(video=0)]\n",
    "        total_frames = len(frame_list)\n",
    "        segment_length = total_frames // n_segments\n",
    "        segments = []\n",
    "        for i in range(n_segments):\n",
    "            start = i * segment_length\n",
    "            end = min(start + segment_length, total_frames)\n",
    "            segment_frames = frame_list[start:end] if end - start == segment_length else frame_list[start:] + [frame_list[-1]] * (segment_length - (end - start))\n",
    "            segments.append(segment_frames[:frames_per_segment])\n",
    "        return segments\n",
    "\n",
    "    def predict_video_and_segments(self, container, true_label):\n",
    "        video_segments = self.split_video_into_segments(container)\n",
    "        segment_outputs = []\n",
    "        with torch.no_grad():\n",
    "            for segment in video_segments:\n",
    "                inputs = self.image_processor(list(segment), return_tensors=\"pt\")\n",
    "                inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
    "                try:\n",
    "                    outputs = self.model(**inputs)\n",
    "                    logits = outputs.logits\n",
    "                    probabilities = F.softmax(logits, dim=-1)\n",
    "                    pred_label = logits.argmax(-1).item()\n",
    "                    pred_score = probabilities[0, pred_label].item()\n",
    "                    segment_outputs.append((pred_label, pred_score, probabilities))\n",
    "                except RuntimeError as e:\n",
    "                    print(f\"Error processing segment: {e}\")\n",
    "                    continue\n",
    "        return segment_outputs\n",
    "\n",
    "class TemporalShap:\n",
    "    def __init__(self, num_samples=100):\n",
    "        self.num_samples = num_samples\n",
    "\n",
    "    def exact_shapley_values(self, segment_outputs, label_index):\n",
    "        n = len(segment_outputs)\n",
    "        shapley_values = [0] * n\n",
    "        all_indices = list(range(n))\n",
    "        for i in all_indices:\n",
    "            marginal_contributions = []\n",
    "            for subset_size in range(n):\n",
    "                subsets = list(combinations([x for x in all_indices if x != i], subset_size))\n",
    "                for subset in subsets:\n",
    "                    subset_prob = torch.zeros_like(segment_outputs[0][2])\n",
    "                    if subset:\n",
    "                        subset_prob = torch.mean(torch.stack([segment_outputs[j][2] for j in subset]), dim=0)\n",
    "                    with_i_prob = (subset_prob * len(subset) + segment_outputs[i][2]) / (len(subset) + 1)\n",
    "                    marginal_contributions.append(with_i_prob[0, label_index].item() - subset_prob[0, label_index].item())\n",
    "            shapley_values[i] = np.mean(marginal_contributions)\n",
    "        return shapley_values\n",
    "\n",
    "class ShapEvaluator:\n",
    "    def __init__(self, video_processor, shap_calculator):\n",
    "        self.video_processor = video_processor\n",
    "        self.shap_calculator = shap_calculator\n",
    "\n",
    "    def calculate_faithfulness(self, video_path, true_label):\n",
    "        container = av.open(video_path)\n",
    "        segment_outputs = self.video_processor.predict_video_and_segments(container, true_label)\n",
    "        if not segment_outputs:\n",
    "            return None\n",
    "\n",
    "        video_probs = torch.mean(torch.stack([output[2] for output in segment_outputs]), dim=0)\n",
    "        original_prediction = video_probs[0, true_label].item()\n",
    "\n",
    "        shapley_values = self.shap_calculator.exact_shapley_values(segment_outputs, true_label)\n",
    "        \n",
    "        # 选择 Shapley 值最高的 50% 的段（最重要的段）\n",
    "        k = len(shapley_values) // 2\n",
    "        most_important_indices = np.argsort(shapley_values)[-k:]\n",
    "        \n",
    "        # 保留不重要的段，删除重要的段\n",
    "        masked_probs = [output[2] for i, output in enumerate(segment_outputs) if i not in most_important_indices]\n",
    "        \n",
    "        if masked_probs:\n",
    "            masked_video_probs = torch.mean(torch.stack(masked_probs), dim=0)\n",
    "            masked_prediction = masked_video_probs[0, true_label].item()\n",
    "        else:\n",
    "            # 如果所有段都被删除，则假设预测是随机的\n",
    "            masked_prediction = 1.0 / video_probs.shape[1]  # 假设是均匀分布\n",
    "\n",
    "        faithfulness = abs(original_prediction - masked_prediction)\n",
    "        return faithfulness\n",
    "\n",
    "    def calculate_monotonicity(self, video_path, true_label):\n",
    "        container = av.open(video_path)\n",
    "        segment_outputs = self.video_processor.predict_video_and_segments(container, true_label)\n",
    "        if not segment_outputs:\n",
    "            return None\n",
    "\n",
    "        shapley_values = self.shap_calculator.exact_shapley_values(segment_outputs, true_label)\n",
    "        \n",
    "        percentages = np.arange(0.1, 1.0, 0.1)\n",
    "        diffs = []\n",
    "\n",
    "        original_probs = torch.mean(torch.stack([output[2] for output in segment_outputs]), dim=0)\n",
    "        original_prediction = original_probs[0, true_label].item()\n",
    "\n",
    "        for p in percentages:\n",
    "            k = int(p * len(shapley_values))\n",
    "            most_important_indices = np.argsort(shapley_values)[-k:]\n",
    "            \n",
    "            masked_probs = [output[2] for i, output in enumerate(segment_outputs) if i not in most_important_indices]\n",
    "            \n",
    "            if masked_probs:\n",
    "                masked_video_probs = torch.mean(torch.stack(masked_probs), dim=0)\n",
    "                masked_prediction = masked_video_probs[0, true_label].item()\n",
    "            else:\n",
    "                masked_prediction = 1.0 / original_probs.shape[1]  # 假设是均匀分布\n",
    "            \n",
    "            diff = abs(original_prediction - masked_prediction)\n",
    "            diffs.append(diff)\n",
    "\n",
    "        tau, _ = kendalltau(percentages, diffs)\n",
    "        return tau\n",
    "\n",
    "def load_video_labels(label_file):\n",
    "    video_labels = {}\n",
    "    with open(label_file, 'r') as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split()\n",
    "            if len(parts) == 2:\n",
    "                video_name, label = parts\n",
    "                video_labels[video_name.split('.')[0]] = int(label)\n",
    "    return video_labels\n",
    "\n",
    "def evaluate_shap(config):\n",
    "    video_processor = VideoProcessor(config[\"model_name\"], config[\"image_processor_name\"])\n",
    "    shap_calculator = TemporalShap(num_samples=config[\"num_samples\"])\n",
    "    evaluator = ShapEvaluator(video_processor, shap_calculator)\n",
    "\n",
    "    video_labels = load_video_labels(config[\"video_list_path\"])\n",
    "    \n",
    "    faithfulness_scores = []\n",
    "    monotonicity_scores = []\n",
    "    computation_times = []\n",
    "\n",
    "    for video_name, true_label in tqdm(list(video_labels.items())[:config[\"num_eval_videos\"]], desc=\"Evaluating videos\"):\n",
    "        video_path = os.path.join(config[\"video_directory\"], video_name + '.mp4')\n",
    "        \n",
    "        start_time = time.time()\n",
    "        \n",
    "        faithfulness = evaluator.calculate_faithfulness(video_path, true_label)\n",
    "        monotonicity = evaluator.calculate_monotonicity(video_path, true_label)\n",
    "        \n",
    "        end_time = time.time()\n",
    "        computation_time = end_time - start_time\n",
    "\n",
    "        if faithfulness is not None and monotonicity is not None:\n",
    "            faithfulness_scores.append(faithfulness)\n",
    "            monotonicity_scores.append(monotonicity)\n",
    "            computation_times.append(computation_time)\n",
    "\n",
    "    results = {\n",
    "        \"faithfulness\": {\n",
    "            \"mean\": np.mean(faithfulness_scores),\n",
    "            \"std\": np.std(faithfulness_scores)\n",
    "        },\n",
    "        \"monotonicity\": {\n",
    "            \"mean\": np.mean(monotonicity_scores),\n",
    "            \"std\": np.std(monotonicity_scores)\n",
    "        },\n",
    "        \"computation_time\": {\n",
    "            \"mean\": np.mean(computation_times),\n",
    "            \"std\": np.std(computation_times)\n",
    "        }\n",
    "    }\n",
    "\n",
    "    with open(\"shap_evaluation_results.json\", \"w\") as f:\n",
    "        json.dump(results, f, indent=4)\n",
    "\n",
    "    print(f\"Faithfulness: {results['faithfulness']['mean']:.4f} ± {results['faithfulness']['std']:.4f}\")\n",
    "    print(f\"Monotonicity: {results['monotonicity']['mean']:.4f} ± {results['monotonicity']['std']:.4f}\")\n",
    "    print(f\"Computation Time: {results['computation_time']['mean']:.2f} ± {results['computation_time']['std']:.2f} seconds\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    config = {\n",
    "        \"model_name\": \"facebook/timesformer-base-finetuned-k400\",\n",
    "        \"image_processor_name\": \"MCG-NJU/videomae-base-finetuned-kinetics\",\n",
    "        \"num_samples\": 100,\n",
    "        \"video_list_path\": \"archive/kinetics400_val_list_videos.txt\",\n",
    "        \"video_directory\": \"archive/videos_val\",\n",
    "        \"num_eval_videos\": 10  # 设置要评估的视频数量\n",
    "    }\n",
    "    \n",
    "    evaluate_shap(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "保留重要的段"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration. Please open a PR/issue to update `preprocessor_config.json` to use `image_processor_type` instead of `feature_extractor_type`. This warning will be removed in v4.40.\n",
      "Evaluating videos: 100%|██████████| 10/10 [00:56<00:00,  5.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faithfulness: 0.8515 ± 0.1543\n",
      "Monotonicity: -0.5353 ± 0.0000\n",
      "Computation Time: 5.70 ± 0.11 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import av\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoImageProcessor, TimesformerForVideoClassification\n",
    "from tqdm import tqdm\n",
    "from scipy.stats import kendalltau\n",
    "import time\n",
    "import json\n",
    "import random\n",
    "from collections import defaultdict\n",
    "\n",
    "class VideoProcessor:\n",
    "    def __init__(self, model_name, image_processor_name, device='cuda'):\n",
    "        self.model = TimesformerForVideoClassification.from_pretrained(model_name)\n",
    "        self.image_processor = AutoImageProcessor.from_pretrained(image_processor_name)\n",
    "        self.device = device\n",
    "        self.model.to(device)\n",
    "\n",
    "    def split_video_into_segments(self, container, n_segments=8, frames_per_segment=16):\n",
    "        frame_list = [frame.to_image() for frame in container.decode(video=0)]\n",
    "        total_frames = len(frame_list)\n",
    "        segment_length = total_frames // n_segments\n",
    "        segments = []\n",
    "        for i in range(n_segments):\n",
    "            start = i * segment_length\n",
    "            end = min(start + segment_length, total_frames)\n",
    "            segment_frames = frame_list[start:end] if end - start == segment_length else frame_list[start:] + [frame_list[-1]] * (segment_length - (end - start))\n",
    "            segments.append(segment_frames[:frames_per_segment])\n",
    "        return segments\n",
    "\n",
    "    def predict_video_and_segments(self, container, true_label):\n",
    "        video_segments = self.split_video_into_segments(container)\n",
    "        segment_outputs = []\n",
    "        with torch.no_grad():\n",
    "            for segment in video_segments:\n",
    "                inputs = self.image_processor(list(segment), return_tensors=\"pt\")\n",
    "                inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
    "                try:\n",
    "                    outputs = self.model(**inputs)\n",
    "                    logits = outputs.logits\n",
    "                    probabilities = F.softmax(logits, dim=-1)\n",
    "                    pred_label = logits.argmax(-1).item()\n",
    "                    pred_score = probabilities[0, pred_label].item()\n",
    "                    segment_outputs.append((pred_label, pred_score, probabilities))\n",
    "                except RuntimeError as e:\n",
    "                    print(f\"Error processing segment: {e}\")\n",
    "                    continue\n",
    "        return segment_outputs\n",
    "\n",
    "class TemporalShap:\n",
    "    def __init__(self, num_samples=100):\n",
    "        self.num_samples = num_samples\n",
    "\n",
    "    def exact_shapley_values(self, segment_outputs, label_index):\n",
    "        n = len(segment_outputs)\n",
    "        shapley_values = [0] * n\n",
    "        all_indices = list(range(n))\n",
    "        for i in all_indices:\n",
    "            marginal_contributions = []\n",
    "            for subset_size in range(n):\n",
    "                subsets = list(combinations([x for x in all_indices if x != i], subset_size))\n",
    "                for subset in subsets:\n",
    "                    subset_prob = torch.zeros_like(segment_outputs[0][2])\n",
    "                    if subset:\n",
    "                        subset_prob = torch.mean(torch.stack([segment_outputs[j][2] for j in subset]), dim=0)\n",
    "                    with_i_prob = (subset_prob * len(subset) + segment_outputs[i][2]) / (len(subset) + 1)\n",
    "                    marginal_contributions.append(with_i_prob[0, label_index].item() - subset_prob[0, label_index].item())\n",
    "            shapley_values[i] = np.mean(marginal_contributions)\n",
    "        return shapley_values\n",
    "\n",
    "class ShapEvaluator:\n",
    "    def __init__(self, video_processor, shap_calculator):\n",
    "        self.video_processor = video_processor\n",
    "        self.shap_calculator = shap_calculator\n",
    "\n",
    "    def calculate_faithfulness(self, video_path, true_label):\n",
    "        container = av.open(video_path)\n",
    "        segment_outputs = self.video_processor.predict_video_and_segments(container, true_label)\n",
    "        if not segment_outputs:\n",
    "            return None\n",
    "\n",
    "        video_probs = torch.mean(torch.stack([output[2] for output in segment_outputs]), dim=0)\n",
    "        original_prediction = video_probs[0, true_label].item()\n",
    "\n",
    "        shapley_values = self.shap_calculator.exact_shapley_values(segment_outputs, true_label)\n",
    "        \n",
    "        # 选择 Shapley 值最高的 50% 的段（最重要的段）\n",
    "        k = len(shapley_values) // 2\n",
    "        most_important_indices = np.argsort(shapley_values)[-k:]\n",
    "        \n",
    "        # 只保留重要的段\n",
    "        important_probs = [output[2] for i, output in enumerate(segment_outputs) if i in most_important_indices]\n",
    "        \n",
    "        if important_probs:\n",
    "            important_video_probs = torch.mean(torch.stack(important_probs), dim=0)\n",
    "            important_prediction = important_video_probs[0, true_label].item()\n",
    "        else:\n",
    "            # 如果没有重要的段，则假设预测是随机的\n",
    "            important_prediction = 1.0 / video_probs.shape[1]  # 假设是均匀分布\n",
    "\n",
    "        faithfulness = 1 - abs(original_prediction - important_prediction)\n",
    "        return faithfulness\n",
    "\n",
    "    def calculate_monotonicity(self, video_path, true_label):\n",
    "        container = av.open(video_path)\n",
    "        segment_outputs = self.video_processor.predict_video_and_segments(container, true_label)\n",
    "        if not segment_outputs:\n",
    "            return None\n",
    "\n",
    "        shapley_values = self.shap_calculator.exact_shapley_values(segment_outputs, true_label)\n",
    "        \n",
    "        percentages = np.arange(0.1, 1.0, 0.1)\n",
    "        diffs = []\n",
    "\n",
    "        original_probs = torch.mean(torch.stack([output[2] for output in segment_outputs]), dim=0)\n",
    "        original_prediction = original_probs[0, true_label].item()\n",
    "\n",
    "        for p in percentages:\n",
    "            k = int(p * len(shapley_values))\n",
    "            most_important_indices = np.argsort(shapley_values)[-k:]\n",
    "            \n",
    "            important_probs = [output[2] for i, output in enumerate(segment_outputs) if i in most_important_indices]\n",
    "            \n",
    "            if important_probs:\n",
    "                important_video_probs = torch.mean(torch.stack(important_probs), dim=0)\n",
    "                important_prediction = important_video_probs[0, true_label].item()\n",
    "            else:\n",
    "                important_prediction = 1.0 / original_probs.shape[1]  # 假设是均匀分布\n",
    "            \n",
    "            diff = abs(original_prediction - important_prediction)\n",
    "            diffs.append(diff)\n",
    "\n",
    "        tau, _ = kendalltau(percentages, diffs)\n",
    "        return tau\n",
    "\n",
    "def load_video_labels(label_file):\n",
    "    video_labels = {}\n",
    "    with open(label_file, 'r') as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split()\n",
    "            if len(parts) == 2:\n",
    "                video_name, label = parts\n",
    "                video_labels[video_name.split('.')[0]] = int(label)\n",
    "    return video_labels\n",
    "\n",
    "def evaluate_shap(config):\n",
    "    video_processor = VideoProcessor(config[\"model_name\"], config[\"image_processor_name\"])\n",
    "    shap_calculator = TemporalShap(num_samples=config[\"num_samples\"])\n",
    "    evaluator = ShapEvaluator(video_processor, shap_calculator)\n",
    "\n",
    "    video_labels = load_video_labels(config[\"video_list_path\"])\n",
    "    \n",
    "    faithfulness_scores = []\n",
    "    monotonicity_scores = []\n",
    "    computation_times = []\n",
    "\n",
    "    for video_name, true_label in tqdm(list(video_labels.items())[:config[\"num_eval_videos\"]], desc=\"Evaluating videos\"):\n",
    "        video_path = os.path.join(config[\"video_directory\"], video_name + '.mp4')\n",
    "        \n",
    "        start_time = time.time()\n",
    "        \n",
    "        faithfulness = evaluator.calculate_faithfulness(video_path, true_label)\n",
    "        monotonicity = evaluator.calculate_monotonicity(video_path, true_label)\n",
    "        \n",
    "        end_time = time.time()\n",
    "        computation_time = end_time - start_time\n",
    "\n",
    "        if faithfulness is not None and monotonicity is not None:\n",
    "            faithfulness_scores.append(faithfulness)\n",
    "            monotonicity_scores.append(monotonicity)\n",
    "            computation_times.append(computation_time)\n",
    "\n",
    "    results = {\n",
    "        \"faithfulness\": {\n",
    "            \"mean\": np.mean(faithfulness_scores),\n",
    "            \"std\": np.std(faithfulness_scores)\n",
    "        },\n",
    "        \"monotonicity\": {\n",
    "            \"mean\": np.mean(monotonicity_scores),\n",
    "            \"std\": np.std(monotonicity_scores)\n",
    "        },\n",
    "        \"computation_time\": {\n",
    "            \"mean\": np.mean(computation_times),\n",
    "            \"std\": np.std(computation_times)\n",
    "        }\n",
    "    }\n",
    "\n",
    "    with open(\"shap_evaluation_results.json\", \"w\") as f:\n",
    "        json.dump(results, f, indent=4)\n",
    "\n",
    "    print(f\"Faithfulness: {results['faithfulness']['mean']:.4f} ± {results['faithfulness']['std']:.4f}\")\n",
    "    print(f\"Monotonicity: {results['monotonicity']['mean']:.4f} ± {results['monotonicity']['std']:.4f}\")\n",
    "    print(f\"Computation Time: {results['computation_time']['mean']:.2f} ± {results['computation_time']['std']:.2f} seconds\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    config = {\n",
    "        \"model_name\": \"facebook/timesformer-base-finetuned-k400\",\n",
    "        \"image_processor_name\": \"MCG-NJU/videomae-base-finetuned-kinetics\",\n",
    "        \"num_samples\": 100,\n",
    "        \"video_list_path\": \"archive/kinetics400_val_list_videos.txt\",\n",
    "        \"video_directory\": \"archive/videos_val\",\n",
    "        \"num_eval_videos\": 10  # 设置要评估的视频数量\n",
    "    }\n",
    "    \n",
    "    evaluate_shap(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LIME\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-02 16:32:02,374 - INFO - Using device: cuda\n",
      "/home/z/miniconda3/envs/mmxai/lib/python3.12/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/home/z/miniconda3/envs/mmxai/lib/python3.12/site-packages/transformers/models/vit/feature_extraction_vit.py:28: FutureWarning: The class ViTFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use ViTImageProcessor instead.\n",
      "  warnings.warn(\n",
      "2024-10-02 16:32:03,555 - INFO - Model and feature extractor loaded successfully\n",
      "Evaluating videos:   0%|          | 0/4 [00:00<?, ?it/s]2024-10-02 16:32:03,566 - INFO - Processing video: archive/example/-8oPwToqArE.mp4 (Label: 265)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c3e2a9adfd3446981c6f77719c88e98",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "803bc238e3bb4b4ea462162950fedcef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-02 16:32:31,825 - WARNING - All difference values are the same, monotonicity is undefined\n",
      "2024-10-02 16:32:31,825 - WARNING - Unable to calculate monotonicity for frame 2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3420865a05c947699b0d3dfd2b9fff61",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-02 16:32:44,633 - WARNING - All difference values are the same, monotonicity is undefined\n",
      "2024-10-02 16:32:44,634 - WARNING - Unable to calculate monotonicity for frame 3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26f2926fd7d74dc5b8eb16a8bc42994c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-02 16:32:57,625 - WARNING - All difference values are the same, monotonicity is undefined\n",
      "2024-10-02 16:32:57,625 - WARNING - Unable to calculate monotonicity for frame 4\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98b5f50c526d4deba8f5fb988e492de8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-02 16:33:10,462 - WARNING - All difference values are the same, monotonicity is undefined\n",
      "2024-10-02 16:33:10,462 - WARNING - Unable to calculate monotonicity for frame 5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd549d1a2b1c4871ab9e9081b8d97a41",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-02 16:33:23,661 - WARNING - All difference values are the same, monotonicity is undefined\n",
      "2024-10-02 16:33:23,662 - WARNING - Unable to calculate monotonicity for frame 6\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a2a5e6614b64f99a3ea98cdea32f73d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-02 16:33:36,387 - WARNING - All difference values are the same, monotonicity is undefined\n",
      "2024-10-02 16:33:36,388 - WARNING - Unable to calculate monotonicity for frame 7\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d91e022b6854b0faf4912c69675a089",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-02 16:33:49,543 - WARNING - All difference values are the same, monotonicity is undefined\n",
      "2024-10-02 16:33:49,543 - WARNING - Unable to calculate monotonicity for frame 8\n",
      "2024-10-02 16:33:49,544 - INFO - Processed -8oPwToqArE.mp4\n",
      "2024-10-02 16:33:49,544 - INFO -   Faithfulness: 0.0144 ± 0.0029\n",
      "2024-10-02 16:33:49,544 - INFO -   Monotonicity: -0.4472 ± 0.0000\n",
      "Evaluating videos:  25%|██▌       | 1/4 [01:45<05:17, 105.98s/it]2024-10-02 16:33:49,545 - INFO - Processing video: archive/example/-6wNVod8iag.mp4 (Label: 241)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dbef5e1426934fc2a22923c34dfe233a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04a9bb1fe8ca4e28b606d4725b49b08a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4dfa61cf364543209d1e57c492ed02d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-02 16:34:19,533 - WARNING - All difference values are the same, monotonicity is undefined\n",
      "2024-10-02 16:34:19,533 - WARNING - Unable to calculate monotonicity for frame 3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2893962b932143dfb672baecfc376c32",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6d73541c8044f2ca834366ba72bdc10",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "237384116e724d9296eb518baaf02dfc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cacf80e97c78429ba5e8cf61de536cb2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dfd94904607a43d78cdb603b1971f97f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-02 16:35:09,366 - INFO - Processed -6wNVod8iag.mp4\n",
      "2024-10-02 16:35:09,366 - INFO -   Faithfulness: 0.2210 ± 0.0163\n",
      "2024-10-02 16:35:09,367 - INFO -   Monotonicity: -0.4472 ± 0.0000\n",
      "Evaluating videos:  50%|█████     | 2/4 [03:05<03:01, 90.59s/it] 2024-10-02 16:35:09,368 - INFO - Processing video: archive/example/-9i4bm2OiZ4.mp4 (Label: 201)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3afcb96655844c90b7c5e89ab16842e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4fc015ccb51d45279836a1367e67f1a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60c02aa0b57d454082a6d5b26ddc3c65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63698f9e7b954be29b4cf9aa90f5e903",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1bdd690ee9e481183e08ef92d4d7945",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d64775629af74e1caf2e809a909a4253",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33f44b03ee5441ae87ad97ca8be64f02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba57ee8cabd04155a24e42ea7fc9b62b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-02 16:36:28,207 - INFO - Processed -9i4bm2OiZ4.mp4\n",
      "2024-10-02 16:36:28,208 - INFO -   Faithfulness: 0.9575 ± 0.0046\n",
      "2024-10-02 16:36:28,208 - INFO -   Monotonicity: -0.4472 ± 0.0000\n",
      "Evaluating videos:  75%|███████▌  | 3/4 [04:24<01:25, 85.23s/it]2024-10-02 16:36:28,209 - INFO - Processing video: archive/example/-0ew-c0w7uc.mp4 (Label: 122)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "043d3f2ec8ee477fb2c41bde602e4275",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5fed9b22517e4da4b213230c8a922a8a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e32a229668948aab83aaf15eb7480a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65d6e2894b8a4bbdba3e219f1da9617a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "456bd4782e3f4d919314cd617776d0fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "406533ab519c4c11946628bfde129934",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3d014606bcb443e8f429928a7a8ea65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "101034338b294c8c982ea66b7d6ea353",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-02 16:37:44,443 - INFO - Processed -0ew-c0w7uc.mp4\n",
      "2024-10-02 16:37:44,443 - INFO -   Faithfulness: 0.3924 ± 0.0037\n",
      "2024-10-02 16:37:44,444 - INFO -   Monotonicity: -0.5404 ± 0.0722\n",
      "Evaluating videos: 100%|██████████| 4/4 [05:40<00:00, 85.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faithfulness: 0.3963 ± 0.3507\n",
      "Monotonicity: -0.4783 ± 0.0605\n",
      "Computation Time: 85.22 ± 12.06 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import av\n",
    "import torch\n",
    "from transformers import ViTForImageClassification, ViTFeatureExtractor, ViTConfig\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from lime import lime_image\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import logging\n",
    "from torch.cuda.amp import autocast\n",
    "from scipy.stats import kendalltau\n",
    "import time\n",
    "\n",
    "# 设置日志\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# Configuration\n",
    "config = {\n",
    "    \"model_config\": \"google/vit-base-patch16-224\",\n",
    "    \"model_path\": \"finetuned_vit_model_20.pth\",\n",
    "    \"feature_extractor_name\": \"google/vit-base-patch16-224\",\n",
    "    \"video_directory\": \"archive/example\",\n",
    "    \"results_folder\": \"archive/exampleLIME\",\n",
    "    \"num_classes\": 400,\n",
    "    \"num_frames_per_video\": 8,\n",
    "    \"lime_num_samples\": 1000,\n",
    "    \"video_list_path\": \"archive/kinetics400_val_list_videos.txt\",\n",
    "    \"num_eval_videos\": 10  # 设置要评估的视频数量，如果想处理所有视频，可以设置为一个很大的数\n",
    "}\n",
    "\n",
    "# 确保结果目录存在\n",
    "os.makedirs(config[\"results_folder\"], exist_ok=True)\n",
    "\n",
    "# 设置设备\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "logging.info(f\"Using device: {device}\")\n",
    "\n",
    "# 加载模型和特征提取器\n",
    "try:\n",
    "    model_config = ViTConfig.from_pretrained(config[\"model_config\"], num_labels=config[\"num_classes\"])\n",
    "    model = ViTForImageClassification(model_config)\n",
    "    model.load_state_dict(torch.load(config[\"model_path\"], map_location=device))\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    feature_extractor = ViTFeatureExtractor.from_pretrained(config[\"feature_extractor_name\"])\n",
    "    logging.info(\"Model and feature extractor loaded successfully\")\n",
    "except Exception as e:\n",
    "    logging.error(f\"Error loading model or feature extractor: {e}\")\n",
    "    raise\n",
    "\n",
    "def load_video_labels(label_file):\n",
    "    video_labels = {}\n",
    "    with open(label_file, 'r') as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split()\n",
    "            if len(parts) == 2:\n",
    "                video_name, label = parts\n",
    "                video_labels[video_name.split('.')[0]] = int(label)\n",
    "    return video_labels\n",
    "\n",
    "def extract_frames(video_path, num_frames):\n",
    "    frames = []\n",
    "    try:\n",
    "        with av.open(video_path) as container:\n",
    "            stream = container.streams.video[0]\n",
    "            duration = stream.duration * stream.time_base\n",
    "            for i in range(num_frames):\n",
    "                target_ts = duration * (i + 1) / (num_frames + 1)\n",
    "                container.seek(int(target_ts / stream.time_base))\n",
    "                for frame in container.decode(video=0):\n",
    "                    frames.append(frame.to_image())\n",
    "                    break\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error extracting frames from {video_path}: {e}\")\n",
    "    return frames\n",
    "\n",
    "def calculate_faithfulness(original_prediction, masked_prediction):\n",
    "    return 1 - abs(original_prediction - masked_prediction)\n",
    "\n",
    "def calculate_monotonicity(percentages, diffs):\n",
    "    if len(percentages) != len(diffs):\n",
    "        logging.warning(f\"Mismatch in lengths: percentages ({len(percentages)}) and diffs ({len(diffs)})\")\n",
    "        return None\n",
    "    \n",
    "    if len(set(diffs)) == 1:  # 如果所有的差异值都相同\n",
    "        logging.warning(\"All difference values are the same, monotonicity is undefined\")\n",
    "        return None\n",
    "    \n",
    "    if np.isnan(diffs).any() or np.isinf(diffs).any():\n",
    "        logging.warning(\"NaN or Inf values found in diffs\")\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        tau, p_value = kendalltau(percentages, diffs)\n",
    "        if np.isnan(tau):\n",
    "            logging.warning(\"Kendall's tau is NaN\")\n",
    "            return None\n",
    "        return tau\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error in calculating Kendall's tau: {e}\")\n",
    "        return None\n",
    "\n",
    "def process_and_explain(video_path, model, feature_extractor, num_frames):\n",
    "    frames = extract_frames(video_path, num_frames)\n",
    "    if not frames:\n",
    "        logging.error(f\"No frames extracted from {video_path}\")\n",
    "        return None\n",
    "\n",
    "    results = []\n",
    "    for i, frame in enumerate(frames):\n",
    "        try:\n",
    "            inputs = feature_extractor(images=frame, return_tensors=\"pt\").to(device)\n",
    "            with autocast():\n",
    "                with torch.no_grad():\n",
    "                    outputs = model(**inputs)\n",
    "            preds = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "            top_pred = preds.argmax().item()\n",
    "            original_prediction = preds[0, top_pred].item()\n",
    "\n",
    "            def batch_predict(images):\n",
    "                batch_inputs = feature_extractor(images=[Image.fromarray(img.astype('uint8')) for img in images], return_tensors=\"pt\").to(device)\n",
    "                with autocast():\n",
    "                    with torch.no_grad():\n",
    "                        batch_outputs = model(**batch_inputs)\n",
    "                return torch.nn.functional.softmax(batch_outputs.logits, dim=-1).cpu().numpy()\n",
    "\n",
    "            explainer = lime_image.LimeImageExplainer()\n",
    "            explanation = explainer.explain_instance(np.array(frame), \n",
    "                                                     batch_predict, \n",
    "                                                     top_labels=5, \n",
    "                                                     hide_color=0, \n",
    "                                                     num_samples=config[\"lime_num_samples\"])\n",
    "            \n",
    "            saliency_map = explanation.get_image_and_mask(top_pred, positive_only=True, num_features=10, hide_rest=False)[1]\n",
    "            saliency_map = (saliency_map - saliency_map.min()) / (saliency_map.max() - saliency_map.min())\n",
    "\n",
    "            # Calculate faithfulness\n",
    "            mask = saliency_map > 0.5\n",
    "            masked_frame = np.array(frame) * mask[..., np.newaxis]\n",
    "            masked_inputs = feature_extractor(images=Image.fromarray(masked_frame.astype('uint8')), return_tensors=\"pt\").to(device)\n",
    "            with autocast():\n",
    "                with torch.no_grad():\n",
    "                    masked_outputs = model(**masked_inputs)\n",
    "            masked_preds = torch.nn.functional.softmax(masked_outputs.logits, dim=-1)\n",
    "            masked_prediction = masked_preds[0, top_pred].item()\n",
    "            faithfulness = calculate_faithfulness(original_prediction, masked_prediction)\n",
    "\n",
    "            # Calculate monotonicity\n",
    "            percentages = np.linspace(0.1, 1.0, 10)\n",
    "            diffs = []\n",
    "            for p in percentages:\n",
    "                threshold = np.percentile(saliency_map, 100 * (1 - p))\n",
    "                temp_mask = saliency_map > threshold\n",
    "                temp_masked_frame = np.array(frame) * temp_mask[..., np.newaxis]\n",
    "                temp_inputs = feature_extractor(images=Image.fromarray(temp_masked_frame.astype('uint8')), return_tensors=\"pt\").to(device)\n",
    "                with autocast():\n",
    "                    with torch.no_grad():\n",
    "                        temp_outputs = model(**temp_inputs)\n",
    "                temp_preds = torch.nn.functional.softmax(temp_outputs.logits, dim=-1)\n",
    "                temp_prediction = temp_preds[0, top_pred].item()\n",
    "                diffs.append(abs(original_prediction - temp_prediction))\n",
    "            \n",
    "            logging.debug(f\"Percentages: {percentages}\")\n",
    "            logging.debug(f\"Diffs: {diffs}\")\n",
    "            \n",
    "            monotonicity = calculate_monotonicity(percentages, diffs)\n",
    "            if monotonicity is None:\n",
    "                logging.warning(f\"Unable to calculate monotonicity for frame {i+1}\")\n",
    "\n",
    "            results.append({\n",
    "                \"frame_index\": i,\n",
    "                \"top_prediction\": top_pred,\n",
    "                \"prediction_score\": original_prediction,\n",
    "                \"faithfulness\": faithfulness,\n",
    "                \"monotonicity\": monotonicity\n",
    "            })\n",
    "\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error processing frame {i+1} of {video_path}: {e}\")\n",
    "\n",
    "    return results\n",
    "\n",
    "def evaluate_lime(config):\n",
    "    video_labels = load_video_labels(config[\"video_list_path\"])\n",
    "    \n",
    "    faithfulness_scores = []\n",
    "    monotonicity_scores = []\n",
    "    computation_times = []\n",
    "\n",
    "    video_files = [f for f in os.listdir(config[\"video_directory\"]) if f.endswith('.mp4')][:config[\"num_eval_videos\"]]\n",
    "\n",
    "    for video_file in tqdm(video_files, desc=\"Evaluating videos\"):\n",
    "        video_path = os.path.join(config[\"video_directory\"], video_file)\n",
    "        video_name = os.path.splitext(video_file)[0]\n",
    "        true_label = video_labels.get(video_name)\n",
    "        \n",
    "        if true_label is None:\n",
    "            logging.warning(f\"Label not found for video: {video_file}\")\n",
    "            continue\n",
    "\n",
    "        logging.info(f\"Processing video: {video_path} (Label: {true_label})\")\n",
    "        \n",
    "        start_time = time.time()\n",
    "        results = process_and_explain(video_path, model, feature_extractor, config[\"num_frames_per_video\"])\n",
    "        end_time = time.time()\n",
    "        \n",
    "        if results:\n",
    "            frame_faithfulness = [r['faithfulness'] for r in results]\n",
    "            frame_monotonicity = [r['monotonicity'] for r in results if r['monotonicity'] is not None]\n",
    "            faithfulness_scores.extend(frame_faithfulness)\n",
    "            monotonicity_scores.extend(frame_monotonicity)\n",
    "            computation_times.append(end_time - start_time)\n",
    "            logging.info(f\"Processed {video_file}\")\n",
    "            logging.info(f\"  Faithfulness: {np.mean(frame_faithfulness):.4f} ± {np.std(frame_faithfulness):.4f}\")\n",
    "            if frame_monotonicity:\n",
    "                logging.info(f\"  Monotonicity: {np.mean(frame_monotonicity):.4f} ± {np.std(frame_monotonicity):.4f}\")\n",
    "            else:\n",
    "                logging.warning(f\"  No valid monotonicity scores for {video_file}\")\n",
    "        else:\n",
    "            logging.warning(f\"Failed to process {video_file}\")\n",
    "\n",
    "    # 计算结果时处理可能的空列表\n",
    "    results = {\n",
    "        \"faithfulness\": {\n",
    "            \"mean\": np.mean(faithfulness_scores) if faithfulness_scores else \"N/A\",\n",
    "            \"std\": np.std(faithfulness_scores) if faithfulness_scores else \"N/A\"\n",
    "        },\n",
    "        \"monotonicity\": {\n",
    "            \"mean\": np.mean(monotonicity_scores) if monotonicity_scores else \"N/A\",\n",
    "            \"std\": np.std(monotonicity_scores) if monotonicity_scores else \"N/A\"\n",
    "        },\n",
    "        \"computation_time\": {\n",
    "            \"mean\": np.mean(computation_times) if computation_times else \"N/A\",\n",
    "            \"std\": np.std(computation_times) if computation_times else \"N/A\"\n",
    "        }\n",
    "    }\n",
    "\n",
    "    with open(os.path.join(config[\"results_folder\"], \"lime_evaluation_results.json\"), \"w\") as f:\n",
    "        json.dump(results, f, indent=4)\n",
    "\n",
    "    print(f\"Faithfulness: {results['faithfulness']['mean']:.4f} ± {results['faithfulness']['std']:.4f}\")\n",
    "    print(f\"Monotonicity: {results['monotonicity']['mean']:.4f} ± {results['monotonicity']['std']:.4f}\")\n",
    "    print(f\"Computation Time: {results['computation_time']['mean']:.2f} ± {results['computation_time']['std']:.2f} seconds\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    evaluate_lime(config)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mmxai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
